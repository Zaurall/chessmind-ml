{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T17:05:56.113810500Z",
     "start_time": "2024-10-06T17:05:49.759821100Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from data import get_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-06T17:05:57.899579100Z",
     "start_time": "2024-10-06T17:05:57.890300800Z"
    }
   },
   "id": "6eb61595c3d08dbf"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optim, iters=25):\n",
    "    model.train()\n",
    "    Loss = []\n",
    "    for _ in range(iters):\n",
    "        input, output = get_batch()\n",
    "        out = model(input)\n",
    "\n",
    "        loss = loss_fn(out, output)\n",
    "        Loss.append(loss.item())\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    return sum(Loss) / len(Loss)\n",
    "\n",
    "\n",
    "def test(model, loss_fn, iters=5):\n",
    "    model.eval()\n",
    "    Loss = []\n",
    "    for _ in range(iters):\n",
    "        input, output = get_batch(test=True)\n",
    "        out = model(input)\n",
    "\n",
    "        loss = loss_fn(out, output)\n",
    "        Loss.append(loss.item())\n",
    "    return sum(Loss) / len(Loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-06T17:05:59.880134100Z",
     "start_time": "2024-10-06T17:05:59.872369600Z"
    }
   },
   "id": "7be751c2e82aa546"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(384, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 64),\n",
    "    nn.Sigmoid()\n",
    ").to(device)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optim = torch.optim.Adam(model.parameters(), 3e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-06T17:06:03.475518700Z",
     "start_time": "2024-10-06T17:06:02.264335200Z"
    }
   },
   "id": "da140f666a319080"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "best = 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-06T17:06:05.072631500Z",
     "start_time": "2024-10-06T17:06:05.063111Z"
    }
   },
   "id": "d9694a2d1d5dfa4f"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1...\n",
      "Train loss: 0.590489696264267\n",
      "Test loss: 0.30027714371681213\n",
      "Model saved\n",
      "Epoch 2...\n",
      "Train loss: 0.19218393564224243\n",
      "Test loss: 0.16832111775875092\n",
      "Model saved\n",
      "Epoch 3...\n",
      "Train loss: 0.15260956764221192\n",
      "Test loss: 0.14461359977722169\n",
      "Model saved\n",
      "Epoch 4...\n",
      "Train loss: 0.14179442703723907\n",
      "Test loss: 0.13934496641159058\n",
      "Model saved\n",
      "Epoch 5...\n",
      "Train loss: 0.14035926043987274\n",
      "Test loss: 0.13655787110328674\n",
      "Model saved\n",
      "Epoch 6...\n",
      "Train loss: 0.13877664208412172\n",
      "Test loss: 0.13705521523952485\n",
      "Epoch 7...\n",
      "Train loss: 0.13790787637233734\n",
      "Test loss: 0.13594107329845428\n",
      "Model saved\n",
      "Epoch 8...\n",
      "Train loss: 0.1371196061372757\n",
      "Test loss: 0.13309445977210999\n",
      "Model saved\n",
      "Epoch 9...\n",
      "Train loss: 0.13400350749492645\n",
      "Test loss: 0.13375204801559448\n",
      "Epoch 10...\n",
      "Train loss: 0.1325165206193924\n",
      "Test loss: 0.1297953099012375\n",
      "Model saved\n",
      "Epoch 11...\n",
      "Train loss: 0.1308620947599411\n",
      "Test loss: 0.12893966734409332\n",
      "Model saved\n",
      "Epoch 12...\n",
      "Train loss: 0.13107216775417327\n",
      "Test loss: 0.13018912672996522\n",
      "Epoch 13...\n",
      "Train loss: 0.12986638724803926\n",
      "Test loss: 0.1300068199634552\n",
      "Epoch 14...\n",
      "Train loss: 0.1305286231637001\n",
      "Test loss: 0.12794197499752044\n",
      "Model saved\n",
      "Epoch 15...\n",
      "Train loss: 0.12809121757745742\n",
      "Test loss: 0.1284128651022911\n",
      "Epoch 16...\n",
      "Train loss: 0.12841513097286225\n",
      "Test loss: 0.12660646438598633\n",
      "Model saved\n",
      "Epoch 17...\n",
      "Train loss: 0.1292266419529915\n",
      "Test loss: 0.12779061794281005\n",
      "Epoch 18...\n",
      "Train loss: 0.12966046512126922\n",
      "Test loss: 0.1283096194267273\n",
      "Epoch 19...\n",
      "Train loss: 0.12956155478954315\n",
      "Test loss: 0.12774267494678498\n",
      "Epoch 20...\n",
      "Train loss: 0.12909355998039246\n",
      "Test loss: 0.12781848907470703\n",
      "Epoch 21...\n",
      "Train loss: 0.12840492725372316\n",
      "Test loss: 0.12453881949186325\n",
      "Model saved\n",
      "Epoch 22...\n",
      "Train loss: 0.12798672109842302\n",
      "Test loss: 0.12474630624055863\n",
      "Epoch 23...\n",
      "Train loss: 0.1286947113275528\n",
      "Test loss: 0.12794296443462372\n",
      "Epoch 24...\n",
      "Train loss: 0.127896089553833\n",
      "Test loss: 0.12800193279981614\n",
      "Epoch 25...\n",
      "Train loss: 0.12812733322381972\n",
      "Test loss: 0.12850728631019592\n",
      "Epoch 26...\n",
      "Train loss: 0.12827609837055207\n",
      "Test loss: 0.12792367339134217\n",
      "Epoch 27...\n",
      "Train loss: 0.12794314801692963\n",
      "Test loss: 0.12836389243602753\n",
      "Epoch 28...\n",
      "Train loss: 0.12727950304746627\n",
      "Test loss: 0.12424875497817993\n",
      "Model saved\n",
      "Epoch 29...\n",
      "Train loss: 0.1279728353023529\n",
      "Test loss: 0.12605572938919068\n",
      "Epoch 30...\n",
      "Train loss: 0.12819782823324202\n",
      "Test loss: 0.12637607306241988\n",
      "Epoch 31...\n",
      "Train loss: 0.12762802153825759\n",
      "Test loss: 0.12422291189432144\n",
      "Model saved\n",
      "Epoch 32...\n",
      "Train loss: 0.12790535092353822\n",
      "Test loss: 0.12832947075366974\n",
      "Epoch 33...\n",
      "Train loss: 0.12789072275161742\n",
      "Test loss: 0.1245782122015953\n",
      "Epoch 34...\n",
      "Train loss: 0.1279901897907257\n",
      "Test loss: 0.12469571232795715\n",
      "Epoch 35...\n",
      "Train loss: 0.12749714463949202\n",
      "Test loss: 0.12518304884433745\n",
      "Epoch 36...\n",
      "Train loss: 0.12736535519361497\n",
      "Test loss: 0.1275320291519165\n",
      "Epoch 37...\n",
      "Train loss: 0.12685408383607866\n",
      "Test loss: 0.12626957893371582\n",
      "Epoch 38...\n",
      "Train loss: 0.12591103821992874\n",
      "Test loss: 0.12479665279388427\n",
      "Epoch 39...\n",
      "Train loss: 0.1271975103020668\n",
      "Test loss: 0.121421979367733\n",
      "Model saved\n",
      "Epoch 40...\n",
      "Train loss: 0.12663430362939834\n",
      "Test loss: 0.12409309297800064\n",
      "Epoch 41...\n",
      "Train loss: 0.12605123728513717\n",
      "Test loss: 0.12716198712587357\n",
      "Epoch 42...\n",
      "Train loss: 0.12689120948314667\n",
      "Test loss: 0.12651503086090088\n",
      "Epoch 43...\n",
      "Train loss: 0.12710055440664292\n",
      "Test loss: 0.12533400654792787\n",
      "Epoch 44...\n",
      "Train loss: 0.12647496193647384\n",
      "Test loss: 0.12483730465173722\n",
      "Epoch 45...\n",
      "Train loss: 0.12565114438533784\n",
      "Test loss: 0.12822917401790618\n",
      "Epoch 46...\n",
      "Train loss: 0.12567212492227553\n",
      "Test loss: 0.1231953263282776\n",
      "Epoch 47...\n",
      "Train loss: 0.12676627933979034\n",
      "Test loss: 0.12228793054819107\n",
      "Epoch 48...\n",
      "Train loss: 0.12644825577735902\n",
      "Test loss: 0.1272813379764557\n",
      "Epoch 49...\n",
      "Train loss: 0.12622844517230988\n",
      "Test loss: 0.12583059817552567\n",
      "Epoch 50...\n",
      "Train loss: 0.1254100087285042\n",
      "Test loss: 0.12645194232463836\n",
      "Epoch 51...\n",
      "Train loss: 0.1273130214214325\n",
      "Test loss: 0.12640763372182845\n",
      "Epoch 52...\n",
      "Train loss: 0.12731449604034423\n",
      "Test loss: 0.12398087829351426\n",
      "Epoch 53...\n",
      "Train loss: 0.1257309013605118\n",
      "Test loss: 0.1263527825474739\n",
      "Epoch 54...\n",
      "Train loss: 0.12680605709552764\n",
      "Test loss: 0.12652591615915298\n",
      "Epoch 55...\n",
      "Train loss: 0.12531628459692\n",
      "Test loss: 0.1267128884792328\n",
      "Epoch 56...\n",
      "Train loss: 0.12625079691410065\n",
      "Test loss: 0.12499662190675735\n",
      "Epoch 57...\n",
      "Train loss: 0.12664722681045532\n",
      "Test loss: 0.12487124353647232\n",
      "Epoch 58...\n",
      "Train loss: 0.12476293653249741\n",
      "Test loss: 0.12539837658405303\n",
      "Epoch 59...\n",
      "Train loss: 0.12534552842378616\n",
      "Test loss: 0.1252373695373535\n",
      "Epoch 60...\n",
      "Train loss: 0.1253516161441803\n",
      "Test loss: 0.12168716490268708\n",
      "Epoch 61...\n",
      "Train loss: 0.12548052459955217\n",
      "Test loss: 0.12537806034088134\n",
      "Epoch 62...\n",
      "Train loss: 0.1256692001223564\n",
      "Test loss: 0.12264191806316375\n",
      "Epoch 63...\n",
      "Train loss: 0.12469885319471359\n",
      "Test loss: 0.1256899356842041\n",
      "Epoch 64...\n",
      "Train loss: 0.12526464581489563\n",
      "Test loss: 0.12502696961164475\n",
      "Epoch 65...\n",
      "Train loss: 0.12540604829788207\n",
      "Test loss: 0.12669078707695008\n",
      "Epoch 66...\n",
      "Train loss: 0.12455317974090577\n",
      "Test loss: 0.12560197561979294\n",
      "Epoch 67...\n",
      "Train loss: 0.12566461890935898\n",
      "Test loss: 0.1228128120303154\n",
      "Epoch 68...\n",
      "Train loss: 0.1251631936430931\n",
      "Test loss: 0.12533570677042008\n",
      "Epoch 69...\n",
      "Train loss: 0.12508084505796432\n",
      "Test loss: 0.12198013365268708\n",
      "Epoch 70...\n",
      "Train loss: 0.12415853559970856\n",
      "Test loss: 0.12185777723789215\n",
      "Epoch 71...\n",
      "Train loss: 0.12440873146057128\n",
      "Test loss: 0.1255318209528923\n",
      "Epoch 72...\n",
      "Train loss: 0.12458738178014755\n",
      "Test loss: 0.12489182204008102\n",
      "Epoch 73...\n",
      "Train loss: 0.12239764273166656\n",
      "Test loss: 0.12739171981811523\n",
      "Epoch 74...\n",
      "Train loss: 0.1251709309220314\n",
      "Test loss: 0.1244097262620926\n",
      "Epoch 75...\n",
      "Train loss: 0.12335742741823197\n",
      "Test loss: 0.12168113142251968\n",
      "Epoch 76...\n",
      "Train loss: 0.12280818372964859\n",
      "Test loss: 0.12509190738201142\n",
      "Epoch 77...\n",
      "Train loss: 0.12276726424694061\n",
      "Test loss: 0.12130036354064941\n",
      "Model saved\n",
      "Epoch 78...\n",
      "Train loss: 0.1230502888560295\n",
      "Test loss: 0.12271225452423096\n",
      "Epoch 79...\n",
      "Train loss: 0.12203209310770034\n",
      "Test loss: 0.12061702609062194\n",
      "Model saved\n",
      "Epoch 80...\n",
      "Train loss: 0.12376072853803635\n",
      "Test loss: 0.12226814478635788\n",
      "Epoch 81...\n",
      "Train loss: 0.12289134621620178\n",
      "Test loss: 0.12502187192440034\n",
      "Epoch 82...\n",
      "Train loss: 0.12381757378578186\n",
      "Test loss: 0.11891558021306992\n",
      "Model saved\n",
      "Epoch 83...\n",
      "Train loss: 0.12180871188640595\n",
      "Test loss: 0.12432690262794495\n",
      "Epoch 84...\n",
      "Train loss: 0.1228737223148346\n",
      "Test loss: 0.12196484804153443\n",
      "Epoch 85...\n",
      "Train loss: 0.12312619596719741\n",
      "Test loss: 0.12362165302038193\n",
      "Epoch 86...\n",
      "Train loss: 0.1214922434091568\n",
      "Test loss: 0.12300917357206345\n",
      "Epoch 87...\n",
      "Train loss: 0.12230569005012512\n",
      "Test loss: 0.12730123549699784\n",
      "Epoch 88...\n",
      "Train loss: 0.12364012837409973\n",
      "Test loss: 0.12311028242111206\n",
      "Epoch 89...\n",
      "Train loss: 0.12158228427171708\n",
      "Test loss: 0.12618976980447769\n",
      "Epoch 90...\n",
      "Train loss: 0.12265520423650741\n",
      "Test loss: 0.12196388840675354\n",
      "Epoch 91...\n",
      "Train loss: 0.12295558661222458\n",
      "Test loss: 0.12370037883520127\n",
      "Epoch 92...\n",
      "Train loss: 0.12280668407678604\n",
      "Test loss: 0.12398190051317215\n",
      "Epoch 93...\n",
      "Train loss: 0.12225565612316132\n",
      "Test loss: 0.1271612301468849\n",
      "Epoch 94...\n",
      "Train loss: 0.12115526646375656\n",
      "Test loss: 0.12354728430509568\n",
      "Epoch 95...\n",
      "Train loss: 0.12153540462255479\n",
      "Test loss: 0.11977896690368653\n",
      "Epoch 96...\n",
      "Train loss: 0.12174548387527466\n",
      "Test loss: 0.12502150386571884\n",
      "Epoch 97...\n",
      "Train loss: 0.12155493646860123\n",
      "Test loss: 0.12208216935396195\n",
      "Epoch 98...\n",
      "Train loss: 0.12227757424116134\n",
      "Test loss: 0.11835030317306519\n",
      "Model saved\n",
      "Epoch 99...\n",
      "Train loss: 0.12118419706821441\n",
      "Test loss: 0.12113572508096696\n",
      "Epoch 100...\n",
      "Train loss: 0.11956115514039993\n",
      "Test loss: 0.12250373512506485\n",
      "Epoch 101...\n",
      "Train loss: 0.12071330010890961\n",
      "Test loss: 0.1208850622177124\n",
      "Epoch 102...\n",
      "Train loss: 0.12107443422079087\n",
      "Test loss: 0.12137284278869628\n",
      "Epoch 103...\n",
      "Train loss: 0.12061635673046112\n",
      "Test loss: 0.12025022506713867\n",
      "Epoch 104...\n",
      "Train loss: 0.12085207670927048\n",
      "Test loss: 0.12471649050712585\n",
      "Epoch 105...\n",
      "Train loss: 0.12136091738939285\n",
      "Test loss: 0.12255553305149078\n",
      "Epoch 106...\n",
      "Train loss: 0.12003283619880677\n",
      "Test loss: 0.12269998341798782\n",
      "Epoch 107...\n",
      "Train loss: 0.1203665891289711\n",
      "Test loss: 0.12329080551862717\n",
      "Epoch 108...\n",
      "Train loss: 0.12143063724040985\n",
      "Test loss: 0.11863157600164413\n",
      "Epoch 109...\n",
      "Train loss: 0.1201548883318901\n",
      "Test loss: 0.12385516166687012\n",
      "Epoch 110...\n",
      "Train loss: 0.1205606946349144\n",
      "Test loss: 0.119729183614254\n",
      "Epoch 111...\n",
      "Train loss: 0.1192528447508812\n",
      "Test loss: 0.11959109902381897\n",
      "Epoch 112...\n",
      "Train loss: 0.1203271746635437\n",
      "Test loss: 0.125848588347435\n",
      "Epoch 113...\n",
      "Train loss: 0.11972756028175353\n",
      "Test loss: 0.1194868266582489\n",
      "Epoch 114...\n",
      "Train loss: 0.11901660054922104\n",
      "Test loss: 0.12413362860679626\n",
      "Epoch 115...\n",
      "Train loss: 0.11868566155433655\n",
      "Test loss: 0.11987953186035157\n",
      "Epoch 116...\n",
      "Train loss: 0.11877663373947143\n",
      "Test loss: 0.12217897027730942\n",
      "Epoch 117...\n",
      "Train loss: 0.11847576349973679\n",
      "Test loss: 0.12036197632551193\n",
      "Epoch 118...\n",
      "Train loss: 0.11959710657596588\n",
      "Test loss: 0.12186965644359589\n",
      "Epoch 119...\n",
      "Train loss: 0.11945171982049942\n",
      "Test loss: 0.11983292102813721\n",
      "Epoch 120...\n",
      "Train loss: 0.11936783879995345\n",
      "Test loss: 0.11793282926082611\n",
      "Model saved\n",
      "Epoch 121...\n",
      "Train loss: 0.11877329289913177\n",
      "Test loss: 0.12150396257638932\n",
      "Epoch 122...\n",
      "Train loss: 0.11891003578901291\n",
      "Test loss: 0.11955548822879791\n",
      "Epoch 123...\n",
      "Train loss: 0.11854125559329987\n",
      "Test loss: 0.12077929824590683\n",
      "Epoch 124...\n",
      "Train loss: 0.11784347772598266\n",
      "Test loss: 0.1202617570757866\n",
      "Epoch 125...\n",
      "Train loss: 0.1174381560087204\n",
      "Test loss: 0.12358143031597138\n",
      "Epoch 126...\n",
      "Train loss: 0.11685312479734421\n",
      "Test loss: 0.12066893726587295\n",
      "Epoch 127...\n",
      "Train loss: 0.11817543089389801\n",
      "Test loss: 0.11995137482881546\n",
      "Epoch 128...\n",
      "Train loss: 0.11762812077999114\n",
      "Test loss: 0.11944106817245484\n",
      "Epoch 129...\n",
      "Train loss: 0.11771445870399475\n",
      "Test loss: 0.12101598083972931\n",
      "Epoch 130...\n",
      "Train loss: 0.11734513938426971\n",
      "Test loss: 0.11813245862722396\n",
      "Epoch 131...\n",
      "Train loss: 0.11763679265975951\n",
      "Test loss: 0.12179299741983414\n",
      "Epoch 132...\n",
      "Train loss: 0.11810541599988937\n",
      "Test loss: 0.12293572723865509\n",
      "Epoch 133...\n",
      "Train loss: 0.11840957254171372\n",
      "Test loss: 0.12208659499883652\n",
      "Epoch 134...\n",
      "Train loss: 0.11673389881849289\n",
      "Test loss: 0.11864184439182282\n",
      "Epoch 135...\n",
      "Train loss: 0.11478078424930573\n",
      "Test loss: 0.1225219264626503\n",
      "Epoch 136...\n",
      "Train loss: 0.11648718625307083\n",
      "Test loss: 0.12468986064195633\n",
      "Epoch 137...\n",
      "Train loss: 0.11740624248981475\n",
      "Test loss: 0.12038309872150421\n",
      "Epoch 138...\n",
      "Train loss: 0.11714423447847366\n",
      "Test loss: 0.1235843613743782\n",
      "Epoch 139...\n",
      "Train loss: 0.11732904255390167\n",
      "Test loss: 0.1175295889377594\n",
      "Model saved\n",
      "Epoch 140...\n",
      "Train loss: 0.11537182092666626\n",
      "Test loss: 0.12389022260904312\n",
      "Epoch 141...\n",
      "Train loss: 0.11545983642339706\n",
      "Test loss: 0.11898407936096192\n",
      "Epoch 142...\n",
      "Train loss: 0.11441299110651017\n",
      "Test loss: 0.11820995956659316\n",
      "Epoch 143...\n",
      "Train loss: 0.1151424840092659\n",
      "Test loss: 0.11781178712844849\n",
      "Epoch 144...\n",
      "Train loss: 0.1155315813422203\n",
      "Test loss: 0.11876794248819351\n",
      "Epoch 145...\n",
      "Train loss: 0.11641049146652221\n",
      "Test loss: 0.11674422025680542\n",
      "Model saved\n",
      "Epoch 146...\n",
      "Train loss: 0.1161604717373848\n",
      "Test loss: 0.11657628118991852\n",
      "Model saved\n",
      "Epoch 147...\n",
      "Train loss: 0.11700125277042389\n",
      "Test loss: 0.11880478411912918\n",
      "Epoch 148...\n",
      "Train loss: 0.11488185286521911\n",
      "Test loss: 0.12344880998134614\n",
      "Epoch 149...\n",
      "Train loss: 0.1145650339126587\n",
      "Test loss: 0.1233532965183258\n",
      "Epoch 150...\n",
      "Train loss: 0.11661885738372803\n",
      "Test loss: 0.12154257297515869\n",
      "Epoch 151...\n",
      "Train loss: 0.1130004608631134\n",
      "Test loss: 0.11567319482564926\n",
      "Model saved\n",
      "Epoch 152...\n",
      "Train loss: 0.11539643973112107\n",
      "Test loss: 0.11571181565523148\n",
      "Epoch 153...\n",
      "Train loss: 0.11489394456148147\n",
      "Test loss: 0.12328735888004302\n",
      "Epoch 154...\n",
      "Train loss: 0.11632159620523452\n",
      "Test loss: 0.11684397608041763\n",
      "Epoch 155...\n",
      "Train loss: 0.11568236261606217\n",
      "Test loss: 0.12354613989591598\n",
      "Epoch 156...\n",
      "Train loss: 0.11519280344247818\n",
      "Test loss: 0.12038827985525132\n",
      "Epoch 157...\n",
      "Train loss: 0.11569349348545074\n",
      "Test loss: 0.12073262184858322\n",
      "Epoch 158...\n",
      "Train loss: 0.11566210716962814\n",
      "Test loss: 0.11731965243816375\n",
      "Epoch 159...\n",
      "Train loss: 0.11369351834058762\n",
      "Test loss: 0.11909714192152024\n",
      "Epoch 160...\n",
      "Train loss: 0.1150791373848915\n",
      "Test loss: 0.12437825798988342\n",
      "Epoch 161...\n",
      "Train loss: 0.11407933920621872\n",
      "Test loss: 0.11723632514476776\n",
      "Epoch 162...\n",
      "Train loss: 0.11342123419046402\n",
      "Test loss: 0.11992010474205017\n",
      "Epoch 163...\n",
      "Train loss: 0.11386369436979293\n",
      "Test loss: 0.11972454190254211\n",
      "Epoch 164...\n",
      "Train loss: 0.11539781779050827\n",
      "Test loss: 0.12310865521430969\n",
      "Epoch 165...\n",
      "Train loss: 0.11416899830102921\n",
      "Test loss: 0.12082015424966812\n",
      "Epoch 166...\n",
      "Train loss: 0.1129677426815033\n",
      "Test loss: 0.11972583383321762\n",
      "Epoch 167...\n",
      "Train loss: 0.1136596491932869\n",
      "Test loss: 0.11618418246507645\n",
      "Epoch 168...\n",
      "Train loss: 0.11299551278352737\n",
      "Test loss: 0.1200863003730774\n",
      "Epoch 169...\n",
      "Train loss: 0.11599217891693116\n",
      "Test loss: 0.11949233412742614\n",
      "Epoch 170...\n",
      "Train loss: 0.1140815755724907\n",
      "Test loss: 0.11874039471149445\n",
      "Epoch 171...\n",
      "Train loss: 0.11262619078159332\n",
      "Test loss: 0.11792508959770202\n",
      "Epoch 172...\n",
      "Train loss: 0.11239192098379135\n",
      "Test loss: 0.1221460983157158\n",
      "Epoch 173...\n",
      "Train loss: 0.1122293695807457\n",
      "Test loss: 0.11649341285228729\n",
      "Epoch 174...\n",
      "Train loss: 0.11341064661741257\n",
      "Test loss: 0.11958924680948257\n",
      "Epoch 175...\n",
      "Train loss: 0.11400793224573136\n",
      "Test loss: 0.11755408942699433\n",
      "Epoch 176...\n",
      "Train loss: 0.11198142349720001\n",
      "Test loss: 0.11846096813678741\n",
      "Epoch 177...\n",
      "Train loss: 0.11254993349313736\n",
      "Test loss: 0.12419269233942032\n",
      "Epoch 178...\n",
      "Train loss: 0.11407441884279251\n",
      "Test loss: 0.11849112212657928\n",
      "Epoch 179...\n",
      "Train loss: 0.11234207600355148\n",
      "Test loss: 0.11640695631504058\n",
      "Epoch 180...\n",
      "Train loss: 0.11219504624605178\n",
      "Test loss: 0.11652138531208038\n",
      "Epoch 181...\n",
      "Train loss: 0.1124169147014618\n",
      "Test loss: 0.11690958142280579\n",
      "Epoch 182...\n",
      "Train loss: 0.11137689352035522\n",
      "Test loss: 0.11664525717496872\n",
      "Epoch 183...\n",
      "Train loss: 0.11179193884134292\n",
      "Test loss: 0.11743314266204834\n",
      "Epoch 184...\n",
      "Train loss: 0.11287560641765594\n",
      "Test loss: 0.11909619122743606\n",
      "Epoch 185...\n",
      "Train loss: 0.1126410698890686\n",
      "Test loss: 0.11631759703159332\n",
      "Epoch 186...\n",
      "Train loss: 0.11094900846481323\n",
      "Test loss: 0.1166999563574791\n",
      "Epoch 187...\n",
      "Train loss: 0.11161945760250092\n",
      "Test loss: 0.11912726908922196\n",
      "Epoch 188...\n",
      "Train loss: 0.11273495137691497\n",
      "Test loss: 0.11606169790029526\n",
      "Epoch 189...\n",
      "Train loss: 0.11271925151348114\n",
      "Test loss: 0.1190178006887436\n",
      "Epoch 190...\n",
      "Train loss: 0.11106280118227005\n",
      "Test loss: 0.11707722842693329\n",
      "Epoch 191...\n",
      "Train loss: 0.11072193741798401\n",
      "Test loss: 0.11780298948287964\n",
      "Epoch 192...\n",
      "Train loss: 0.10938843488693237\n",
      "Test loss: 0.1167928010225296\n",
      "Epoch 193...\n",
      "Train loss: 0.11091536670923233\n",
      "Test loss: 0.1169113889336586\n",
      "Epoch 194...\n",
      "Train loss: 0.11097271114587784\n",
      "Test loss: 0.11879972070455551\n",
      "Epoch 195...\n",
      "Train loss: 0.11021552413702011\n",
      "Test loss: 0.12053898423910141\n",
      "Epoch 196...\n",
      "Train loss: 0.11171069383621215\n",
      "Test loss: 0.11761879175901413\n",
      "Epoch 197...\n",
      "Train loss: 0.11155843675136566\n",
      "Test loss: 0.12331756353378295\n",
      "Epoch 198...\n",
      "Train loss: 0.11055989384651184\n",
      "Test loss: 0.12199916690587997\n",
      "Epoch 199...\n",
      "Train loss: 0.11169877618551255\n",
      "Test loss: 0.11942331045866013\n",
      "Epoch 200...\n",
      "Train loss: 0.10973598659038544\n",
      "Test loss: 0.11619117707014084\n",
      "Epoch 201...\n",
      "Train loss: 0.1116799122095108\n",
      "Test loss: 0.12173361629247666\n",
      "Epoch 202...\n",
      "Train loss: 0.10876330822706222\n",
      "Test loss: 0.12350012958049775\n",
      "Epoch 203...\n",
      "Train loss: 0.11156210631132125\n",
      "Test loss: 0.11698233038187027\n",
      "Epoch 204...\n",
      "Train loss: 0.1091522553563118\n",
      "Test loss: 0.12197652459144592\n",
      "Epoch 205...\n",
      "Train loss: 0.10983237355947495\n",
      "Test loss: 0.11716759949922562\n",
      "Epoch 206...\n",
      "Train loss: 0.1097065931558609\n",
      "Test loss: 0.11616536378860473\n",
      "Epoch 207...\n",
      "Train loss: 0.11094935804605484\n",
      "Test loss: 0.11686753928661346\n",
      "Epoch 208...\n",
      "Train loss: 0.1086121490597725\n",
      "Test loss: 0.11908328980207443\n",
      "Epoch 209...\n",
      "Train loss: 0.11102075457572937\n",
      "Test loss: 0.1132605642080307\n",
      "Model saved\n",
      "Epoch 210...\n",
      "Train loss: 0.11073218703269959\n",
      "Test loss: 0.11852179914712906\n",
      "Epoch 211...\n",
      "Train loss: 0.10829126954078674\n",
      "Test loss: 0.11781761348247528\n",
      "Epoch 212...\n",
      "Train loss: 0.10911650478839874\n",
      "Test loss: 0.11843392997980118\n",
      "Epoch 213...\n",
      "Train loss: 0.10815479546785355\n",
      "Test loss: 0.11301121711730958\n",
      "Model saved\n",
      "Epoch 214...\n",
      "Train loss: 0.10902046233415603\n",
      "Test loss: 0.11778969615697861\n",
      "Epoch 215...\n",
      "Train loss: 0.1088253578543663\n",
      "Test loss: 0.12150738388299942\n",
      "Epoch 216...\n",
      "Train loss: 0.10995501518249512\n",
      "Test loss: 0.12321544289588929\n",
      "Epoch 217...\n",
      "Train loss: 0.11054545402526855\n",
      "Test loss: 0.11758557111024856\n",
      "Epoch 218...\n",
      "Train loss: 0.10779907941818237\n",
      "Test loss: 0.12008043229579926\n",
      "Epoch 219...\n",
      "Train loss: 0.11018721878528595\n",
      "Test loss: 0.11508671194314957\n",
      "Epoch 220...\n",
      "Train loss: 0.10882871955633164\n",
      "Test loss: 0.11561235040426254\n",
      "Epoch 221...\n",
      "Train loss: 0.10944172203540802\n",
      "Test loss: 0.11721663326025009\n",
      "Epoch 222...\n",
      "Train loss: 0.11067405343055725\n",
      "Test loss: 0.11625532060861588\n",
      "Epoch 223...\n",
      "Train loss: 0.10731354922056198\n",
      "Test loss: 0.12108747363090515\n",
      "Epoch 224...\n",
      "Train loss: 0.10922501206398011\n",
      "Test loss: 0.11765564531087876\n",
      "Epoch 225...\n",
      "Train loss: 0.10653165429830551\n",
      "Test loss: 0.11612511128187179\n",
      "Epoch 226...\n",
      "Train loss: 0.10815028488636017\n",
      "Test loss: 0.11335963308811188\n",
      "Epoch 227...\n",
      "Train loss: 0.10927590042352676\n",
      "Test loss: 0.11497504115104676\n",
      "Epoch 228...\n",
      "Train loss: 0.10671570301055908\n",
      "Test loss: 0.11737639755010605\n",
      "Epoch 229...\n",
      "Train loss: 0.110775688290596\n",
      "Test loss: 0.11911234557628632\n",
      "Epoch 230...\n",
      "Train loss: 0.10790919750928879\n",
      "Test loss: 0.11781469881534576\n",
      "Epoch 231...\n",
      "Train loss: 0.10872436553239823\n",
      "Test loss: 0.11680899560451508\n",
      "Epoch 232...\n",
      "Train loss: 0.10746198147535324\n",
      "Test loss: 0.11615072935819626\n",
      "Epoch 233...\n",
      "Train loss: 0.10913753062486649\n",
      "Test loss: 0.11590166687965393\n",
      "Epoch 234...\n",
      "Train loss: 0.10851814299821853\n",
      "Test loss: 0.11538096219301223\n",
      "Epoch 235...\n",
      "Train loss: 0.10827533066272736\n",
      "Test loss: 0.1139254629611969\n",
      "Epoch 236...\n",
      "Train loss: 0.10676277607679367\n",
      "Test loss: 0.12254562079906464\n",
      "Epoch 237...\n",
      "Train loss: 0.10632137984037399\n",
      "Test loss: 0.12141915708780289\n",
      "Epoch 238...\n",
      "Train loss: 0.10778844147920609\n",
      "Test loss: 0.12075760215520859\n",
      "Epoch 239...\n",
      "Train loss: 0.10714572817087173\n",
      "Test loss: 0.12128344476222992\n",
      "Epoch 240...\n",
      "Train loss: 0.10775955021381378\n",
      "Test loss: 0.11922499388456345\n",
      "Epoch 241...\n",
      "Train loss: 0.10865578591823578\n",
      "Test loss: 0.12036043107509613\n",
      "Epoch 242...\n",
      "Train loss: 0.10670156598091125\n",
      "Test loss: 0.11782030016183853\n",
      "Epoch 243...\n",
      "Train loss: 0.10762587964534759\n",
      "Test loss: 0.11678317189216614\n",
      "Epoch 244...\n",
      "Train loss: 0.10611147075891494\n",
      "Test loss: 0.12056396305561065\n",
      "Epoch 245...\n",
      "Train loss: 0.10573489725589752\n",
      "Test loss: 0.1189396172761917\n",
      "Epoch 246...\n",
      "Train loss: 0.10669455409049988\n",
      "Test loss: 0.11522506773471833\n",
      "Epoch 247...\n",
      "Train loss: 0.10562463283538819\n",
      "Test loss: 0.11782996654510498\n",
      "Epoch 248...\n",
      "Train loss: 0.10592836111783982\n",
      "Test loss: 0.11807706952095032\n",
      "Epoch 249...\n",
      "Train loss: 0.1056142008304596\n",
      "Test loss: 0.1159015268087387\n",
      "Epoch 250...\n",
      "Train loss: 0.10758230060338975\n",
      "Test loss: 0.12173882573843002\n",
      "Epoch 251...\n",
      "Train loss: 0.10661674439907073\n",
      "Test loss: 0.11726290732622147\n",
      "Epoch 252...\n",
      "Train loss: 0.10583180129528046\n",
      "Test loss: 0.11791683882474899\n",
      "Epoch 253...\n",
      "Train loss: 0.1067863154411316\n",
      "Test loss: 0.11670526415109635\n",
      "Epoch 254...\n",
      "Train loss: 0.10388637304306031\n",
      "Test loss: 0.12008354812860489\n",
      "Epoch 255...\n",
      "Train loss: 0.10611182779073715\n",
      "Test loss: 0.12318269014358521\n",
      "Epoch 256...\n",
      "Train loss: 0.10653135150671006\n",
      "Test loss: 0.11452284306287766\n",
      "Epoch 257...\n",
      "Train loss: 0.10698416650295257\n",
      "Test loss: 0.11921835243701935\n",
      "Epoch 258...\n",
      "Train loss: 0.10665788292884827\n",
      "Test loss: 0.11392302662134171\n",
      "Epoch 259...\n",
      "Train loss: 0.10466313511133193\n",
      "Test loss: 0.11400698721408845\n",
      "Epoch 260...\n",
      "Train loss: 0.10571809500455856\n",
      "Test loss: 0.1169506311416626\n",
      "Epoch 261...\n",
      "Train loss: 0.10518713772296906\n",
      "Test loss: 0.12071501761674881\n",
      "Epoch 262...\n",
      "Train loss: 0.10610800206661225\n",
      "Test loss: 0.1141608327627182\n",
      "Epoch 263...\n",
      "Train loss: 0.10491543292999267\n",
      "Test loss: 0.12216167896986008\n",
      "Epoch 264...\n",
      "Train loss: 0.10624642878770828\n",
      "Test loss: 0.11947044730186462\n",
      "Epoch 265...\n",
      "Train loss: 0.1041027557849884\n",
      "Test loss: 0.11889706552028656\n",
      "Epoch 266...\n",
      "Train loss: 0.10599975496530532\n",
      "Test loss: 0.1173392117023468\n",
      "Epoch 267...\n",
      "Train loss: 0.10399153709411621\n",
      "Test loss: 0.1226967766880989\n",
      "Epoch 268...\n",
      "Train loss: 0.10391249865293503\n",
      "Test loss: 0.11897046267986297\n",
      "Epoch 269...\n",
      "Train loss: 0.10494539111852647\n",
      "Test loss: 0.11834698021411896\n",
      "Epoch 270...\n",
      "Train loss: 0.1053878065943718\n",
      "Test loss: 0.1201392337679863\n",
      "Epoch 271...\n",
      "Train loss: 0.1037639969587326\n",
      "Test loss: 0.12399744391441345\n",
      "Epoch 272...\n",
      "Train loss: 0.10711974680423736\n",
      "Test loss: 0.12262828946113587\n",
      "Epoch 273...\n",
      "Train loss: 0.10473291099071502\n",
      "Test loss: 0.12166102677583694\n",
      "Epoch 274...\n",
      "Train loss: 0.10532687425613403\n",
      "Test loss: 0.11847199201583862\n",
      "Epoch 275...\n",
      "Train loss: 0.10646921962499618\n",
      "Test loss: 0.12147066742181778\n",
      "Epoch 276...\n",
      "Train loss: 0.10501346051692963\n",
      "Test loss: 0.11743378788232803\n",
      "Epoch 277...\n",
      "Train loss: 0.1061940398812294\n",
      "Test loss: 0.11768288165330887\n",
      "Epoch 278...\n",
      "Train loss: 0.10512758642435074\n",
      "Test loss: 0.11836515516042709\n",
      "Epoch 279...\n",
      "Train loss: 0.10500335723161697\n",
      "Test loss: 0.11833309084177017\n",
      "Epoch 280...\n",
      "Train loss: 0.10413919478654861\n",
      "Test loss: 0.11948195546865463\n",
      "Epoch 281...\n",
      "Train loss: 0.104263054728508\n",
      "Test loss: 0.114464670419693\n",
      "Epoch 282...\n",
      "Train loss: 0.10346955806016922\n",
      "Test loss: 0.12100531905889511\n",
      "Epoch 283...\n",
      "Train loss: 0.10419198662042618\n",
      "Test loss: 0.12306897193193436\n",
      "Epoch 284...\n",
      "Train loss: 0.10252663552761078\n",
      "Test loss: 0.11521517187356949\n",
      "Epoch 285...\n",
      "Train loss: 0.10285089164972305\n",
      "Test loss: 0.11681428104639054\n",
      "Epoch 286...\n",
      "Train loss: 0.10251811057329178\n",
      "Test loss: 0.11633368581533432\n",
      "Epoch 287...\n",
      "Train loss: 0.10452346086502075\n",
      "Test loss: 0.1110693097114563\n",
      "Model saved\n",
      "Epoch 288...\n",
      "Train loss: 0.10273496806621552\n",
      "Test loss: 0.12021756172180176\n",
      "Epoch 289...\n",
      "Train loss: 0.1021935710310936\n",
      "Test loss: 0.12553901374340057\n",
      "Epoch 290...\n",
      "Train loss: 0.10413115561008453\n",
      "Test loss: 0.11824826151132584\n",
      "Epoch 291...\n",
      "Train loss: 0.1040861439704895\n",
      "Test loss: 0.12029824405908585\n",
      "Epoch 292...\n",
      "Train loss: 0.10275221437215805\n",
      "Test loss: 0.11913595795631408\n",
      "Epoch 293...\n",
      "Train loss: 0.10587351500988007\n",
      "Test loss: 0.12306480258703231\n",
      "Epoch 294...\n",
      "Train loss: 0.10190476894378662\n",
      "Test loss: 0.11736950278282166\n",
      "Epoch 295...\n",
      "Train loss: 0.10276761025190354\n",
      "Test loss: 0.11704968512058259\n",
      "Epoch 296...\n",
      "Train loss: 0.10483308613300324\n",
      "Test loss: 0.11481064409017563\n",
      "Epoch 297...\n",
      "Train loss: 0.10229175418615341\n",
      "Test loss: 0.11468926817178726\n",
      "Epoch 298...\n",
      "Train loss: 0.10360358148813248\n",
      "Test loss: 0.11587088704109191\n",
      "Epoch 299...\n",
      "Train loss: 0.1017545372247696\n",
      "Test loss: 0.11553364098072053\n",
      "Epoch 300...\n",
      "Train loss: 0.10225169211626053\n",
      "Test loss: 0.11961563825607299\n",
      "Epoch 301...\n",
      "Train loss: 0.1022383627295494\n",
      "Test loss: 0.11782758384943008\n",
      "Epoch 302...\n",
      "Train loss: 0.10266333609819413\n",
      "Test loss: 0.12016441524028779\n",
      "Epoch 303...\n",
      "Train loss: 0.10309754580259323\n",
      "Test loss: 0.12066118270158768\n",
      "Epoch 304...\n",
      "Train loss: 0.10073558986186981\n",
      "Test loss: 0.11135741323232651\n",
      "Epoch 305...\n",
      "Train loss: 0.1028786090016365\n",
      "Test loss: 0.11400790810585022\n",
      "Epoch 306...\n",
      "Train loss: 0.10319596529006958\n",
      "Test loss: 0.11508825570344924\n",
      "Epoch 307...\n",
      "Train loss: 0.10011365324258804\n",
      "Test loss: 0.12024987787008286\n",
      "Epoch 308...\n",
      "Train loss: 0.10195655703544616\n",
      "Test loss: 0.11704490780830383\n",
      "Epoch 309...\n",
      "Train loss: 0.10223107755184174\n",
      "Test loss: 0.11753368377685547\n",
      "Epoch 310...\n",
      "Train loss: 0.10330436795949936\n",
      "Test loss: 0.11636269390583039\n",
      "Epoch 311...\n",
      "Train loss: 0.1025265783071518\n",
      "Test loss: 0.11844292730093002\n",
      "Epoch 312...\n",
      "Train loss: 0.10230114638805389\n",
      "Test loss: 0.11717990636825562\n",
      "Epoch 313...\n",
      "Train loss: 0.10106277227401733\n",
      "Test loss: 0.11905840784311295\n",
      "Epoch 314...\n",
      "Train loss: 0.1007049748301506\n",
      "Test loss: 0.11724502295255661\n",
      "Epoch 315...\n",
      "Train loss: 0.09977688163518905\n",
      "Test loss: 0.11620631963014602\n",
      "Epoch 316...\n",
      "Train loss: 0.10334708988666534\n",
      "Test loss: 0.11579013168811798\n",
      "Epoch 317...\n",
      "Train loss: 0.10306126415729523\n",
      "Test loss: 0.11641499251127244\n",
      "Epoch 318...\n",
      "Train loss: 0.10169327825307846\n",
      "Test loss: 0.11623524427413941\n",
      "Epoch 319...\n",
      "Train loss: 0.09953820139169693\n",
      "Test loss: 0.11812012642621994\n",
      "Epoch 320...\n",
      "Train loss: 0.10170180529356003\n",
      "Test loss: 0.11761256903409958\n",
      "Epoch 321...\n",
      "Train loss: 0.10032951176166534\n",
      "Test loss: 0.11719905436038972\n",
      "Epoch 322...\n",
      "Train loss: 0.1002402076125145\n",
      "Test loss: 0.11330590695142746\n",
      "Epoch 323...\n",
      "Train loss: 0.09999257385730743\n",
      "Test loss: 0.11586964279413223\n",
      "Epoch 324...\n",
      "Train loss: 0.10235838502645493\n",
      "Test loss: 0.113923479616642\n",
      "Epoch 325...\n",
      "Train loss: 0.10231116384267808\n",
      "Test loss: 0.12025978267192841\n",
      "Epoch 326...\n",
      "Train loss: 0.10002535343170166\n",
      "Test loss: 0.12106276899576188\n",
      "Epoch 327...\n",
      "Train loss: 0.10202393174171448\n",
      "Test loss: 0.11600634902715683\n",
      "Epoch 328...\n",
      "Train loss: 0.10161387532949448\n",
      "Test loss: 0.11708738952875138\n",
      "Epoch 329...\n",
      "Train loss: 0.09994228512048721\n",
      "Test loss: 0.11902868002653122\n",
      "Epoch 330...\n",
      "Train loss: 0.10266886621713639\n",
      "Test loss: 0.11486892849206924\n",
      "Epoch 331...\n",
      "Train loss: 0.10178666859865189\n",
      "Test loss: 0.12248573303222657\n",
      "Epoch 332...\n",
      "Train loss: 0.0997977152466774\n",
      "Test loss: 0.11576127260923386\n",
      "Epoch 333...\n",
      "Train loss: 0.10156779438257217\n",
      "Test loss: 0.1228989765048027\n",
      "Epoch 334...\n",
      "Train loss: 0.10070960879325867\n",
      "Test loss: 0.12084563821554184\n",
      "Epoch 335...\n",
      "Train loss: 0.09987959891557693\n",
      "Test loss: 0.11836071908473969\n",
      "Epoch 336...\n",
      "Train loss: 0.10148275732994079\n",
      "Test loss: 0.12186000943183899\n",
      "Epoch 337...\n",
      "Train loss: 0.1005756476521492\n",
      "Test loss: 0.1154157817363739\n",
      "Epoch 338...\n",
      "Train loss: 0.100090551674366\n",
      "Test loss: 0.11834501922130584\n",
      "Epoch 339...\n",
      "Train loss: 0.09938615709543228\n",
      "Test loss: 0.11696804314851761\n",
      "Epoch 340...\n",
      "Train loss: 0.0986169484257698\n",
      "Test loss: 0.11589600443840027\n",
      "Epoch 341...\n",
      "Train loss: 0.10006096690893174\n",
      "Test loss: 0.11601757556200028\n",
      "Epoch 342...\n",
      "Train loss: 0.09827929884195327\n",
      "Test loss: 0.12055005729198456\n",
      "Epoch 343...\n",
      "Train loss: 0.09954506307840347\n",
      "Test loss: 0.11804342567920685\n",
      "Epoch 344...\n",
      "Train loss: 0.09988485157489777\n",
      "Test loss: 0.11688259989023209\n",
      "Epoch 345...\n",
      "Train loss: 0.09906398743391037\n",
      "Test loss: 0.11824769377708436\n",
      "Epoch 346...\n",
      "Train loss: 0.09794144868850709\n",
      "Test loss: 0.1185896337032318\n",
      "Epoch 347...\n",
      "Train loss: 0.09927314281463623\n",
      "Test loss: 0.11803238540887832\n",
      "Epoch 348...\n",
      "Train loss: 0.09918796062469483\n",
      "Test loss: 0.11593454480171203\n",
      "Epoch 349...\n",
      "Train loss: 0.09884584933519364\n",
      "Test loss: 0.11478792726993561\n",
      "Epoch 350...\n",
      "Train loss: 0.10019318282604217\n",
      "Test loss: 0.11588190793991089\n",
      "Epoch 351...\n",
      "Train loss: 0.0986535108089447\n",
      "Test loss: 0.12069589346647262\n",
      "Epoch 352...\n",
      "Train loss: 0.10077331960201263\n",
      "Test loss: 0.11925024390220643\n",
      "Epoch 353...\n",
      "Train loss: 0.09808861315250397\n",
      "Test loss: 0.11828986406326295\n",
      "Epoch 354...\n",
      "Train loss: 0.0979155632853508\n",
      "Test loss: 0.11414523273706437\n",
      "Epoch 355...\n",
      "Train loss: 0.09912390261888504\n",
      "Test loss: 0.11513421535491944\n",
      "Epoch 356...\n",
      "Train loss: 0.10088809370994568\n",
      "Test loss: 0.11770429164171219\n",
      "Epoch 357...\n",
      "Train loss: 0.09876152873039246\n",
      "Test loss: 0.12097839117050171\n",
      "Epoch 358...\n",
      "Train loss: 0.09999286472797393\n",
      "Test loss: 0.11656444668769836\n",
      "Epoch 359...\n",
      "Train loss: 0.099564368724823\n",
      "Test loss: 0.12644770592451096\n",
      "Epoch 360...\n",
      "Train loss: 0.09889769732952118\n",
      "Test loss: 0.11800165623426437\n",
      "Epoch 361...\n",
      "Train loss: 0.09964632630348205\n",
      "Test loss: 0.11931399554014206\n",
      "Epoch 362...\n",
      "Train loss: 0.09778304010629654\n",
      "Test loss: 0.12085440456867218\n",
      "Epoch 363...\n",
      "Train loss: 0.09698261439800263\n",
      "Test loss: 0.118464395403862\n",
      "Epoch 364...\n",
      "Train loss: 0.09890981316566468\n",
      "Test loss: 0.11667998284101486\n",
      "Epoch 365...\n",
      "Train loss: 0.09864741384983063\n",
      "Test loss: 0.12167651206254959\n",
      "Epoch 366...\n",
      "Train loss: 0.0988865140080452\n",
      "Test loss: 0.1216143399477005\n",
      "Epoch 367...\n",
      "Train loss: 0.09908525943756104\n",
      "Test loss: 0.11899913251399993\n",
      "Epoch 368...\n",
      "Train loss: 0.09930520355701447\n",
      "Test loss: 0.11638367772102357\n",
      "Epoch 369...\n",
      "Train loss: 0.09639880478382111\n",
      "Test loss: 0.11934120208024979\n",
      "Epoch 370...\n",
      "Train loss: 0.09532665640115738\n",
      "Test loss: 0.12227243334054946\n",
      "Epoch 371...\n",
      "Train loss: 0.09737706452608108\n",
      "Test loss: 0.12097624987363816\n",
      "Epoch 372...\n",
      "Train loss: 0.09741996079683304\n",
      "Test loss: 0.11902483105659485\n",
      "Epoch 373...\n",
      "Train loss: 0.0974136883020401\n",
      "Test loss: 0.11440845727920532\n",
      "Epoch 374...\n",
      "Train loss: 0.09903616905212402\n",
      "Test loss: 0.11789109408855439\n",
      "Epoch 375...\n",
      "Train loss: 0.09534139305353165\n",
      "Test loss: 0.11262489855289459\n",
      "Epoch 376...\n",
      "Train loss: 0.09770330756902695\n",
      "Test loss: 0.12322372943162918\n",
      "Epoch 377...\n",
      "Train loss: 0.09838181912899018\n",
      "Test loss: 0.1148517221212387\n",
      "Epoch 378...\n",
      "Train loss: 0.09906015276908875\n",
      "Test loss: 0.11675381660461426\n",
      "Epoch 379...\n",
      "Train loss: 0.09748547464609146\n",
      "Test loss: 0.11499091982841492\n",
      "Epoch 380...\n",
      "Train loss: 0.09557412624359131\n",
      "Test loss: 0.1170034646987915\n",
      "Epoch 381...\n",
      "Train loss: 0.09702412337064743\n",
      "Test loss: 0.11611323356628418\n",
      "Epoch 382...\n",
      "Train loss: 0.09578285247087479\n",
      "Test loss: 0.11870587319135666\n",
      "Epoch 383...\n",
      "Train loss: 0.09721427589654923\n",
      "Test loss: 0.11726683527231216\n",
      "Epoch 384...\n",
      "Train loss: 0.09561285585165023\n",
      "Test loss: 0.11795627027750015\n",
      "Epoch 385...\n",
      "Train loss: 0.0969069692492485\n",
      "Test loss: 0.115074323117733\n",
      "Epoch 386...\n",
      "Train loss: 0.09633569419384003\n",
      "Test loss: 0.12388461083173752\n",
      "Epoch 387...\n",
      "Train loss: 0.09815406203269958\n",
      "Test loss: 0.1271410718560219\n",
      "Epoch 388...\n",
      "Train loss: 0.09597375690937042\n",
      "Test loss: 0.1178288921713829\n",
      "Epoch 389...\n",
      "Train loss: 0.09797489941120148\n",
      "Test loss: 0.11742381900548934\n",
      "Epoch 390...\n",
      "Train loss: 0.09630262017250062\n",
      "Test loss: 0.11745091527700424\n",
      "Epoch 391...\n",
      "Train loss: 0.09816457808017731\n",
      "Test loss: 0.11768786609172821\n",
      "Epoch 392...\n",
      "Train loss: 0.09670531392097473\n",
      "Test loss: 0.11761076003313065\n",
      "Epoch 393...\n",
      "Train loss: 0.09314083188772201\n",
      "Test loss: 0.12178730517625809\n",
      "Epoch 394...\n",
      "Train loss: 0.09481647104024887\n",
      "Test loss: 0.11985093057155609\n",
      "Epoch 395...\n",
      "Train loss: 0.09758291363716126\n",
      "Test loss: 0.12039322108030319\n",
      "Epoch 396...\n",
      "Train loss: 0.09590782105922699\n",
      "Test loss: 0.12096579521894454\n",
      "Epoch 397...\n",
      "Train loss: 0.09584627628326416\n",
      "Test loss: 0.11667632907629014\n",
      "Epoch 398...\n",
      "Train loss: 0.09824382215738296\n",
      "Test loss: 0.11684186160564422\n",
      "Epoch 399...\n",
      "Train loss: 0.0935498121380806\n",
      "Test loss: 0.12399608045816421\n",
      "Epoch 400...\n",
      "Train loss: 0.09379733920097351\n",
      "Test loss: 0.12401228696107865\n",
      "Epoch 401...\n",
      "Train loss: 0.09811583518981934\n",
      "Test loss: 0.12147982120513916\n",
      "Epoch 402...\n",
      "Train loss: 0.09626023769378662\n",
      "Test loss: 0.11549880206584931\n",
      "Epoch 403...\n",
      "Train loss: 0.0949145558476448\n",
      "Test loss: 0.11278495341539382\n",
      "Epoch 404...\n",
      "Train loss: 0.09738803029060364\n",
      "Test loss: 0.11977750360965729\n",
      "Epoch 405...\n",
      "Train loss: 0.09578121095895767\n",
      "Test loss: 0.12084165811538697\n",
      "Epoch 406...\n",
      "Train loss: 0.0920928254723549\n",
      "Test loss: 0.11481340676546097\n",
      "Epoch 407...\n",
      "Train loss: 0.09387514114379883\n",
      "Test loss: 0.11894099116325378\n",
      "Epoch 408...\n",
      "Train loss: 0.09667298763990402\n",
      "Test loss: 0.12463471293449402\n",
      "Epoch 409...\n",
      "Train loss: 0.09539452821016312\n",
      "Test loss: 0.1224621832370758\n",
      "Epoch 410...\n",
      "Train loss: 0.09341103613376617\n",
      "Test loss: 0.11843668818473815\n",
      "Epoch 411...\n",
      "Train loss: 0.09490099191665649\n",
      "Test loss: 0.11527821719646454\n",
      "Epoch 412...\n",
      "Train loss: 0.09515523940324783\n",
      "Test loss: 0.12102868407964706\n",
      "Epoch 413...\n",
      "Train loss: 0.09280019074678421\n",
      "Test loss: 0.12075446397066117\n",
      "Epoch 414...\n",
      "Train loss: 0.09766972810029984\n",
      "Test loss: 0.11964792907238006\n",
      "Epoch 415...\n",
      "Train loss: 0.0951015117764473\n",
      "Test loss: 0.11850771754980087\n",
      "Epoch 416...\n",
      "Train loss: 0.09388215005397797\n",
      "Test loss: 0.11762452274560928\n",
      "Epoch 417...\n",
      "Train loss: 0.0945159751176834\n",
      "Test loss: 0.12152319401502609\n",
      "Epoch 418...\n",
      "Train loss: 0.09500807911157608\n",
      "Test loss: 0.11817397326231002\n",
      "Epoch 419...\n",
      "Train loss: 0.09331714689731598\n",
      "Test loss: 0.11651556044816971\n",
      "Epoch 420...\n",
      "Train loss: 0.09677562177181244\n",
      "Test loss: 0.11790826916694641\n",
      "Epoch 421...\n",
      "Train loss: 0.09129063844680786\n",
      "Test loss: 0.12028467208147049\n",
      "Epoch 422...\n",
      "Train loss: 0.09489595592021942\n",
      "Test loss: 0.12005917727947235\n",
      "Epoch 423...\n",
      "Train loss: 0.09409649521112443\n",
      "Test loss: 0.11893634349107743\n",
      "Epoch 424...\n",
      "Train loss: 0.09341625779867173\n",
      "Test loss: 0.11786212772130966\n",
      "Epoch 425...\n",
      "Train loss: 0.09679525136947632\n",
      "Test loss: 0.12343949377536774\n",
      "Epoch 426...\n",
      "Train loss: 0.0958807721734047\n",
      "Test loss: 0.1202627032995224\n",
      "Epoch 427...\n",
      "Train loss: 0.09478704571723938\n",
      "Test loss: 0.11616089642047882\n",
      "Epoch 428...\n",
      "Train loss: 0.09413867712020874\n",
      "Test loss: 0.12205374389886856\n",
      "Epoch 429...\n",
      "Train loss: 0.09406487494707108\n",
      "Test loss: 0.11866268664598464\n",
      "Epoch 430...\n",
      "Train loss: 0.09349759727716446\n",
      "Test loss: 0.1198483794927597\n",
      "Epoch 431...\n",
      "Train loss: 0.09483327478170395\n",
      "Test loss: 0.11732348948717117\n",
      "Epoch 432...\n",
      "Train loss: 0.093671655356884\n",
      "Test loss: 0.125645150244236\n",
      "Epoch 433...\n",
      "Train loss: 0.09406595677137375\n",
      "Test loss: 0.11905270367860794\n",
      "Epoch 434...\n",
      "Train loss: 0.09255367875099182\n",
      "Test loss: 0.11957796812057495\n",
      "Epoch 435...\n",
      "Train loss: 0.09360351145267487\n",
      "Test loss: 0.12043703496456146\n",
      "Epoch 436...\n",
      "Train loss: 0.09262714684009551\n",
      "Test loss: 0.12287513315677642\n",
      "Epoch 437...\n",
      "Train loss: 0.09350757390260696\n",
      "Test loss: 0.12027515172958374\n",
      "Epoch 438...\n",
      "Train loss: 0.09125928938388825\n",
      "Test loss: 0.119259312748909\n",
      "Epoch 439...\n",
      "Train loss: 0.09404146522283555\n",
      "Test loss: 0.12033531218767166\n",
      "Epoch 440...\n",
      "Train loss: 0.09385590761899948\n",
      "Test loss: 0.12270502001047134\n",
      "Epoch 441...\n",
      "Train loss: 0.09258638918399811\n",
      "Test loss: 0.11635182797908783\n",
      "Epoch 442...\n",
      "Train loss: 0.09469040364027023\n",
      "Test loss: 0.1144450142979622\n",
      "Epoch 443...\n",
      "Train loss: 0.09344055116176606\n",
      "Test loss: 0.11560802012681962\n",
      "Epoch 444...\n",
      "Train loss: 0.0919507583975792\n",
      "Test loss: 0.12055719941854477\n",
      "Epoch 445...\n",
      "Train loss: 0.09222251117229462\n",
      "Test loss: 0.12087263464927674\n",
      "Epoch 446...\n",
      "Train loss: 0.09253772884607316\n",
      "Test loss: 0.11743168830871582\n",
      "Epoch 447...\n",
      "Train loss: 0.0906898581981659\n",
      "Test loss: 0.1270118772983551\n",
      "Epoch 448...\n",
      "Train loss: 0.09474276751279831\n",
      "Test loss: 0.12137766182422638\n",
      "Epoch 449...\n",
      "Train loss: 0.09148325324058533\n",
      "Test loss: 0.11882752180099487\n",
      "Epoch 450...\n",
      "Train loss: 0.09321548700332642\n",
      "Test loss: 0.12181482017040253\n",
      "Epoch 451...\n",
      "Train loss: 0.09463641285896301\n",
      "Test loss: 0.1151439905166626\n",
      "Epoch 452...\n",
      "Train loss: 0.09133699715137482\n",
      "Test loss: 0.11868108659982682\n",
      "Epoch 453...\n",
      "Train loss: 0.09253490060567855\n",
      "Test loss: 0.11603243499994279\n",
      "Epoch 454...\n",
      "Train loss: 0.09111545503139495\n",
      "Test loss: 0.12612988352775573\n",
      "Epoch 455...\n",
      "Train loss: 0.09273303568363189\n",
      "Test loss: 0.1276865556836128\n",
      "Epoch 456...\n",
      "Train loss: 0.09192976057529449\n",
      "Test loss: 0.11900589913129807\n",
      "Epoch 457...\n",
      "Train loss: 0.09159006893634797\n",
      "Test loss: 0.12429121136665344\n",
      "Epoch 458...\n",
      "Train loss: 0.09339810252189636\n",
      "Test loss: 0.1253512144088745\n",
      "Epoch 459...\n",
      "Train loss: 0.0934276282787323\n",
      "Test loss: 0.11798221468925477\n",
      "Epoch 460...\n",
      "Train loss: 0.09370510637760163\n",
      "Test loss: 0.12183109223842621\n",
      "Epoch 461...\n",
      "Train loss: 0.09056888192892075\n",
      "Test loss: 0.1230265200138092\n",
      "Epoch 462...\n",
      "Train loss: 0.09426855742931366\n",
      "Test loss: 0.1169716775417328\n",
      "Epoch 463...\n",
      "Train loss: 0.09362335979938508\n",
      "Test loss: 0.11727684885263442\n",
      "Epoch 464...\n",
      "Train loss: 0.09303758472204209\n",
      "Test loss: 0.11472659856081009\n",
      "Epoch 465...\n",
      "Train loss: 0.08929506599903107\n",
      "Test loss: 0.1258092537522316\n",
      "Epoch 466...\n",
      "Train loss: 0.09239441335201264\n",
      "Test loss: 0.12144288122653961\n",
      "Epoch 467...\n",
      "Train loss: 0.09176796555519104\n",
      "Test loss: 0.12149722427129746\n",
      "Epoch 468...\n",
      "Train loss: 0.09301509767770767\n",
      "Test loss: 0.11883786469697952\n",
      "Epoch 469...\n",
      "Train loss: 0.09168659061193467\n",
      "Test loss: 0.12442200779914855\n",
      "Epoch 470...\n",
      "Train loss: 0.09238056778907776\n",
      "Test loss: 0.11417896449565887\n",
      "Epoch 471...\n",
      "Train loss: 0.09217893332242966\n",
      "Test loss: 0.11803324222564697\n",
      "Epoch 472...\n",
      "Train loss: 0.0912549689412117\n",
      "Test loss: 0.12586768865585327\n",
      "Epoch 473...\n",
      "Train loss: 0.09235378473997116\n",
      "Test loss: 0.11708450317382812\n",
      "Epoch 474...\n",
      "Train loss: 0.0909760057926178\n",
      "Test loss: 0.12228055149316788\n",
      "Epoch 475...\n",
      "Train loss: 0.09127882987260819\n",
      "Test loss: 0.12392281293869019\n",
      "Epoch 476...\n",
      "Train loss: 0.09035311162471771\n",
      "Test loss: 0.1275234952569008\n",
      "Epoch 477...\n",
      "Train loss: 0.09220340996980667\n",
      "Test loss: 0.12055351585149765\n",
      "Epoch 478...\n",
      "Train loss: 0.0911388835310936\n",
      "Test loss: 0.11269160062074661\n",
      "Epoch 479...\n",
      "Train loss: 0.09025400400161743\n",
      "Test loss: 0.11880068182945251\n",
      "Epoch 480...\n",
      "Train loss: 0.09185097217559815\n",
      "Test loss: 0.12114248126745224\n",
      "Epoch 481...\n",
      "Train loss: 0.09119742929935455\n",
      "Test loss: 0.12476081848144531\n",
      "Epoch 482...\n",
      "Train loss: 0.09289792686700821\n",
      "Test loss: 0.12429865896701812\n",
      "Epoch 483...\n",
      "Train loss: 0.0910548558831215\n",
      "Test loss: 0.11050704568624496\n",
      "Model saved\n",
      "Epoch 484...\n",
      "Train loss: 0.0907572877407074\n",
      "Test loss: 0.12586537152528762\n",
      "Epoch 485...\n",
      "Train loss: 0.09221824765205383\n",
      "Test loss: 0.11587458401918412\n",
      "Epoch 486...\n",
      "Train loss: 0.08924628287553787\n",
      "Test loss: 0.11996571570634842\n",
      "Epoch 487...\n",
      "Train loss: 0.09213564753532409\n",
      "Test loss: 0.12274307906627654\n",
      "Epoch 488...\n",
      "Train loss: 0.09076554656028747\n",
      "Test loss: 0.11767904609441757\n",
      "Epoch 489...\n",
      "Train loss: 0.09025312721729278\n",
      "Test loss: 0.11949397921562195\n",
      "Epoch 490...\n",
      "Train loss: 0.08921346426010132\n",
      "Test loss: 0.12013404220342636\n",
      "Epoch 491...\n",
      "Train loss: 0.09172819077968597\n",
      "Test loss: 0.11964975148439408\n",
      "Epoch 492...\n",
      "Train loss: 0.09139411717653274\n",
      "Test loss: 0.12081970572471619\n",
      "Epoch 493...\n",
      "Train loss: 0.09105960965156555\n",
      "Test loss: 0.11891644895076751\n",
      "Epoch 494...\n",
      "Train loss: 0.08907987922430038\n",
      "Test loss: 0.13300551772117614\n",
      "Epoch 495...\n",
      "Train loss: 0.08951249659061432\n",
      "Test loss: 0.12699851989746094\n",
      "Epoch 496...\n",
      "Train loss: 0.08916938483715058\n",
      "Test loss: 0.12464659959077835\n",
      "Epoch 497...\n",
      "Train loss: 0.08897741883993149\n",
      "Test loss: 0.11971033960580826\n",
      "Epoch 498...\n",
      "Train loss: 0.08919983983039856\n",
      "Test loss: 0.12197520732879638\n",
      "Epoch 499...\n",
      "Train loss: 0.09038186043500901\n",
      "Test loss: 0.12840428203344345\n",
      "Epoch 500...\n",
      "Train loss: 0.08825999706983566\n",
      "Test loss: 0.12063240110874177\n",
      "Epoch 501...\n",
      "Train loss: 0.08983565121889114\n",
      "Test loss: 0.11814752370119094\n",
      "Epoch 502...\n",
      "Train loss: 0.09122550040483475\n",
      "Test loss: 0.1255818247795105\n",
      "Epoch 503...\n",
      "Train loss: 0.09047193646430969\n",
      "Test loss: 0.12484068721532822\n",
      "Epoch 504...\n",
      "Train loss: 0.08962639659643173\n",
      "Test loss: 0.12013532668352127\n",
      "Epoch 505...\n",
      "Train loss: 0.08924124836921692\n",
      "Test loss: 0.11953157782554627\n",
      "Epoch 506...\n",
      "Train loss: 0.08950159430503846\n",
      "Test loss: 0.12065845131874084\n",
      "Epoch 507...\n",
      "Train loss: 0.08875448137521744\n",
      "Test loss: 0.12026232928037643\n",
      "Epoch 508...\n",
      "Train loss: 0.09087861686944962\n",
      "Test loss: 0.12202231287956238\n",
      "Epoch 509...\n",
      "Train loss: 0.09001477301120758\n",
      "Test loss: 0.12553496062755584\n",
      "Epoch 510...\n",
      "Train loss: 0.0896794232726097\n",
      "Test loss: 0.1174983486533165\n",
      "Epoch 511...\n",
      "Train loss: 0.09046613365411758\n",
      "Test loss: 0.1202483132481575\n",
      "Epoch 512...\n",
      "Train loss: 0.09007446229457855\n",
      "Test loss: 0.11904616951942444\n",
      "Epoch 513...\n",
      "Train loss: 0.08757137149572372\n",
      "Test loss: 0.12675009816884994\n",
      "Epoch 514...\n",
      "Train loss: 0.08747109979391098\n",
      "Test loss: 0.12762183994054793\n",
      "Epoch 515...\n",
      "Train loss: 0.08768843650817872\n",
      "Test loss: 0.12380247712135314\n",
      "Epoch 516...\n",
      "Train loss: 0.09000415384769439\n",
      "Test loss: 0.1217613935470581\n",
      "Epoch 517...\n",
      "Train loss: 0.09116191983222961\n",
      "Test loss: 0.12060085237026215\n",
      "Epoch 518...\n",
      "Train loss: 0.0879673957824707\n",
      "Test loss: 0.12932670414447783\n",
      "Epoch 519...\n",
      "Train loss: 0.09089875638484955\n",
      "Test loss: 0.12544683516025543\n",
      "Epoch 520...\n",
      "Train loss: 0.08867164522409439\n",
      "Test loss: 0.12021168917417527\n",
      "Epoch 521...\n",
      "Train loss: 0.08746864795684814\n",
      "Test loss: 0.12072842717170715\n",
      "Epoch 522...\n",
      "Train loss: 0.0904511833190918\n",
      "Test loss: 0.12249108105897903\n",
      "Epoch 523...\n",
      "Train loss: 0.08903861969709397\n",
      "Test loss: 0.12148201018571854\n",
      "Epoch 524...\n",
      "Train loss: 0.09068888545036316\n",
      "Test loss: 0.1201987937092781\n",
      "Epoch 525...\n",
      "Train loss: 0.0889936089515686\n",
      "Test loss: 0.12005421370267869\n",
      "Epoch 526...\n",
      "Train loss: 0.09078892290592194\n",
      "Test loss: 0.1144614726305008\n",
      "Epoch 527...\n",
      "Train loss: 0.08750174462795257\n",
      "Test loss: 0.11342695504426956\n",
      "Epoch 528...\n",
      "Train loss: 0.08838343739509583\n",
      "Test loss: 0.12006259113550186\n",
      "Epoch 529...\n",
      "Train loss: 0.08706659704446792\n",
      "Test loss: 0.13193488717079163\n",
      "Epoch 530...\n",
      "Train loss: 0.08764308094978332\n",
      "Test loss: 0.11998437643051148\n",
      "Epoch 531...\n",
      "Train loss: 0.08947541743516922\n",
      "Test loss: 0.13074878305196763\n",
      "Epoch 532...\n",
      "Train loss: 0.08513367503881454\n",
      "Test loss: 0.12852482944726945\n",
      "Epoch 533...\n",
      "Train loss: 0.0884082904458046\n",
      "Test loss: 0.12149667888879775\n",
      "Epoch 534...\n",
      "Train loss: 0.08652307599782944\n",
      "Test loss: 0.12754573822021484\n",
      "Epoch 535...\n",
      "Train loss: 0.086018927693367\n",
      "Test loss: 0.12278270274400711\n",
      "Epoch 536...\n",
      "Train loss: 0.08722514271736145\n",
      "Test loss: 0.12306843101978301\n",
      "Epoch 537...\n",
      "Train loss: 0.08752612173557281\n",
      "Test loss: 0.12211844176054001\n",
      "Epoch 538...\n",
      "Train loss: 0.0891431587934494\n",
      "Test loss: 0.12362854331731796\n",
      "Epoch 539...\n",
      "Train loss: 0.08765287220478057\n",
      "Test loss: 0.12432946115732194\n",
      "Epoch 540...\n",
      "Train loss: 0.08761912763118744\n",
      "Test loss: 0.12183267027139663\n",
      "Epoch 541...\n",
      "Train loss: 0.08841095060110092\n",
      "Test loss: 0.12158809900283814\n",
      "Epoch 542...\n",
      "Train loss: 0.08607085168361664\n",
      "Test loss: 0.122610804438591\n",
      "Epoch 543...\n",
      "Train loss: 0.08609306365251541\n",
      "Test loss: 0.1159096509218216\n",
      "Epoch 544...\n",
      "Train loss: 0.08760948568582534\n",
      "Test loss: 0.11928963959217072\n",
      "Epoch 545...\n",
      "Train loss: 0.08807323396205902\n",
      "Test loss: 0.11766901463270188\n",
      "Epoch 546...\n",
      "Train loss: 0.08743631929159164\n",
      "Test loss: 0.1111798718571663\n",
      "Epoch 547...\n",
      "Train loss: 0.08683885335922241\n",
      "Test loss: 0.12039340734481811\n",
      "Epoch 548...\n",
      "Train loss: 0.08801840901374818\n",
      "Test loss: 0.12801306545734406\n",
      "Epoch 549...\n",
      "Train loss: 0.08671932369470596\n",
      "Test loss: 0.1241835743188858\n",
      "Epoch 550...\n",
      "Train loss: 0.0859145799279213\n",
      "Test loss: 0.1222571685910225\n",
      "Epoch 551...\n",
      "Train loss: 0.08789757847785949\n",
      "Test loss: 0.12261573374271392\n",
      "Epoch 552...\n",
      "Train loss: 0.08526496887207032\n",
      "Test loss: 0.1175730362534523\n",
      "Epoch 553...\n",
      "Train loss: 0.08681071132421493\n",
      "Test loss: 0.12329507172107697\n",
      "Epoch 554...\n",
      "Train loss: 0.0858436906337738\n",
      "Test loss: 0.12546785920858383\n",
      "Epoch 555...\n",
      "Train loss: 0.08444178581237793\n",
      "Test loss: 0.1270940363407135\n",
      "Epoch 556...\n",
      "Train loss: 0.08841086238622665\n",
      "Test loss: 0.12487103790044785\n",
      "Epoch 557...\n",
      "Train loss: 0.08626309901475906\n",
      "Test loss: 0.12543838173151017\n",
      "Epoch 558...\n",
      "Train loss: 0.08624537497758865\n",
      "Test loss: 0.12338850051164627\n",
      "Epoch 559...\n",
      "Train loss: 0.08723281383514404\n",
      "Test loss: 0.11950256228446961\n",
      "Epoch 560...\n",
      "Train loss: 0.08540890216827393\n",
      "Test loss: 0.11845088452100754\n",
      "Epoch 561...\n",
      "Train loss: 0.08624425172805786\n",
      "Test loss: 0.12778121381998062\n",
      "Epoch 562...\n",
      "Train loss: 0.08569221675395966\n",
      "Test loss: 0.12476491928100586\n",
      "Epoch 563...\n",
      "Train loss: 0.08854295700788498\n",
      "Test loss: 0.12330232709646224\n",
      "Epoch 564...\n",
      "Train loss: 0.08458605468273163\n",
      "Test loss: 0.13029932528734206\n",
      "Epoch 565...\n",
      "Train loss: 0.08662885129451751\n",
      "Test loss: 0.13475435972213745\n",
      "Epoch 566...\n",
      "Train loss: 0.08573209077119827\n",
      "Test loss: 0.12069051265716553\n",
      "Epoch 567...\n",
      "Train loss: 0.0851297226548195\n",
      "Test loss: 0.12530590444803238\n",
      "Epoch 568...\n",
      "Train loss: 0.08375109493732452\n",
      "Test loss: 0.12272945046424866\n",
      "Epoch 569...\n",
      "Train loss: 0.08525749862194061\n",
      "Test loss: 0.121745365858078\n",
      "Epoch 570...\n",
      "Train loss: 0.08495121389627457\n",
      "Test loss: 0.12398631125688553\n",
      "Epoch 571...\n",
      "Train loss: 0.08350953191518784\n",
      "Test loss: 0.12039029747247695\n",
      "Epoch 572...\n",
      "Train loss: 0.08562962830066681\n",
      "Test loss: 0.12066156566143035\n",
      "Epoch 573...\n",
      "Train loss: 0.08632835179567337\n",
      "Test loss: 0.1279175266623497\n",
      "Epoch 574...\n",
      "Train loss: 0.08571162283420562\n",
      "Test loss: 0.12669670283794404\n",
      "Epoch 575...\n",
      "Train loss: 0.08670313626527787\n",
      "Test loss: 0.11853211671113968\n",
      "Epoch 576...\n",
      "Train loss: 0.08639558643102646\n",
      "Test loss: 0.12176842540502548\n",
      "Epoch 577...\n",
      "Train loss: 0.08636646538972854\n",
      "Test loss: 0.12221324294805527\n",
      "Epoch 578...\n",
      "Train loss: 0.08478657782077789\n",
      "Test loss: 0.12062946557998658\n",
      "Epoch 579...\n",
      "Train loss: 0.08347632855176926\n",
      "Test loss: 0.12346267551183701\n",
      "Epoch 580...\n",
      "Train loss: 0.08576140135526657\n",
      "Test loss: 0.13225825875997543\n",
      "Epoch 581...\n",
      "Train loss: 0.08325407296419143\n",
      "Test loss: 0.12844278514385224\n",
      "Epoch 582...\n",
      "Train loss: 0.0861051195859909\n",
      "Test loss: 0.12450383305549621\n",
      "Epoch 583...\n",
      "Train loss: 0.08670984268188477\n",
      "Test loss: 0.12867606282234192\n",
      "Epoch 584...\n",
      "Train loss: 0.08430711477994919\n",
      "Test loss: 0.12225283682346344\n",
      "Epoch 585...\n",
      "Train loss: 0.08418841630220414\n",
      "Test loss: 0.12042826861143112\n",
      "Epoch 586...\n",
      "Train loss: 0.0875600728392601\n",
      "Test loss: 0.12707096934318543\n",
      "Epoch 587...\n",
      "Train loss: 0.08619524359703064\n",
      "Test loss: 0.12553705275058746\n",
      "Epoch 588...\n",
      "Train loss: 0.08454186737537384\n",
      "Test loss: 0.11994847059249877\n",
      "Epoch 589...\n",
      "Train loss: 0.08555665016174316\n",
      "Test loss: 0.12901758253574372\n",
      "Epoch 590...\n",
      "Train loss: 0.08321460247039796\n",
      "Test loss: 0.11967673003673554\n",
      "Epoch 591...\n",
      "Train loss: 0.08355465650558472\n",
      "Test loss: 0.12031262665987015\n",
      "Epoch 592...\n",
      "Train loss: 0.08356459707021713\n",
      "Test loss: 0.12437163293361664\n",
      "Epoch 593...\n",
      "Train loss: 0.08592857152223587\n",
      "Test loss: 0.12223899513483047\n",
      "Epoch 594...\n",
      "Train loss: 0.0840274766087532\n",
      "Test loss: 0.12737519890069962\n",
      "Epoch 595...\n",
      "Train loss: 0.08250652998685837\n",
      "Test loss: 0.12388058304786682\n",
      "Epoch 596...\n",
      "Train loss: 0.08327322244644166\n",
      "Test loss: 0.12183014899492264\n",
      "Epoch 597...\n",
      "Train loss: 0.08363966077566147\n",
      "Test loss: 0.12187782824039459\n",
      "Epoch 598...\n",
      "Train loss: 0.08175662517547608\n",
      "Test loss: 0.1245758816599846\n",
      "Epoch 599...\n",
      "Train loss: 0.08402318656444549\n",
      "Test loss: 0.12142981588840485\n",
      "Epoch 600...\n",
      "Train loss: 0.08523179739713668\n",
      "Test loss: 0.12514917999505998\n",
      "Epoch 601...\n",
      "Train loss: 0.08458743512630462\n",
      "Test loss: 0.12571273148059844\n",
      "Epoch 602...\n",
      "Train loss: 0.0838125193119049\n",
      "Test loss: 0.1215890109539032\n",
      "Epoch 603...\n",
      "Train loss: 0.08440623760223388\n",
      "Test loss: 0.12401901781558991\n",
      "Epoch 604...\n",
      "Train loss: 0.08296443939208985\n",
      "Test loss: 0.12207487225532532\n",
      "Epoch 605...\n",
      "Train loss: 0.08409074336290359\n",
      "Test loss: 0.12199998646974564\n",
      "Epoch 606...\n",
      "Train loss: 0.08433567494153976\n",
      "Test loss: 0.12758033722639084\n",
      "Epoch 607...\n",
      "Train loss: 0.08502708673477173\n",
      "Test loss: 0.123758664727211\n",
      "Epoch 608...\n",
      "Train loss: 0.08347996205091476\n",
      "Test loss: 0.12066349387168884\n",
      "Epoch 609...\n",
      "Train loss: 0.08221353769302368\n",
      "Test loss: 0.1257070392370224\n",
      "Epoch 610...\n",
      "Train loss: 0.08252719223499298\n",
      "Test loss: 0.11778359562158584\n",
      "Epoch 611...\n",
      "Train loss: 0.08418432354927063\n",
      "Test loss: 0.12356587201356888\n",
      "Epoch 612...\n",
      "Train loss: 0.08410722643136978\n",
      "Test loss: 0.12383649796247483\n",
      "Epoch 613...\n",
      "Train loss: 0.08525411069393157\n",
      "Test loss: 0.12288127392530442\n",
      "Epoch 614...\n",
      "Train loss: 0.08135407775640488\n",
      "Test loss: 0.12347657233476639\n",
      "Epoch 615...\n",
      "Train loss: 0.08433648556470871\n",
      "Test loss: 0.12392921149730682\n",
      "Epoch 616...\n",
      "Train loss: 0.0822581547498703\n",
      "Test loss: 0.12347177714109421\n",
      "Epoch 617...\n",
      "Train loss: 0.08383800864219665\n",
      "Test loss: 0.12157318443059921\n",
      "Epoch 618...\n",
      "Train loss: 0.08354470402002334\n",
      "Test loss: 0.13013477772474288\n",
      "Epoch 619...\n",
      "Train loss: 0.08548605024814605\n",
      "Test loss: 0.13488055467605592\n",
      "Epoch 620...\n",
      "Train loss: 0.08521697014570236\n",
      "Test loss: 0.12791489362716674\n",
      "Epoch 621...\n",
      "Train loss: 0.08326166749000549\n",
      "Test loss: 0.12305018603801728\n",
      "Epoch 622...\n",
      "Train loss: 0.08276426136493682\n",
      "Test loss: 0.12497411519289017\n",
      "Epoch 623...\n",
      "Train loss: 0.08268489480018616\n",
      "Test loss: 0.13025707006454468\n",
      "Epoch 624...\n",
      "Train loss: 0.0838457390666008\n",
      "Test loss: 0.12059967964887619\n",
      "Epoch 625...\n",
      "Train loss: 0.08152133077383042\n",
      "Test loss: 0.12103298604488373\n",
      "Epoch 626...\n",
      "Train loss: 0.08272494167089463\n",
      "Test loss: 0.12508074045181275\n",
      "Epoch 627...\n",
      "Train loss: 0.08134531199932099\n",
      "Test loss: 0.12804533988237382\n",
      "Epoch 628...\n",
      "Train loss: 0.08239032089710235\n",
      "Test loss: 0.11874146461486816\n",
      "Epoch 629...\n",
      "Train loss: 0.08344207480549812\n",
      "Test loss: 0.12583362460136413\n",
      "Epoch 630...\n",
      "Train loss: 0.08284861803054809\n",
      "Test loss: 0.12396529763936996\n",
      "Epoch 631...\n",
      "Train loss: 0.0845273569226265\n",
      "Test loss: 0.12620749622583388\n",
      "Epoch 632...\n",
      "Train loss: 0.08302799731492996\n",
      "Test loss: 0.11934721767902375\n",
      "Epoch 633...\n",
      "Train loss: 0.08330976009368897\n",
      "Test loss: 0.11761116683483124\n",
      "Epoch 634...\n",
      "Train loss: 0.08157522827386857\n",
      "Test loss: 0.12246603518724442\n",
      "Epoch 635...\n",
      "Train loss: 0.0814958569407463\n",
      "Test loss: 0.12709860801696776\n",
      "Epoch 636...\n",
      "Train loss: 0.08369968831539154\n",
      "Test loss: 0.13065271228551864\n",
      "Epoch 637...\n",
      "Train loss: 0.07921094626188278\n",
      "Test loss: 0.12628515362739562\n",
      "Epoch 638...\n",
      "Train loss: 0.08099960565567016\n",
      "Test loss: 0.13153406381607055\n",
      "Epoch 639...\n",
      "Train loss: 0.08189644932746887\n",
      "Test loss: 0.1237246036529541\n",
      "Epoch 640...\n",
      "Train loss: 0.08202678948640824\n",
      "Test loss: 0.1385196715593338\n",
      "Epoch 641...\n",
      "Train loss: 0.08155063599348068\n",
      "Test loss: 0.12670422494411468\n",
      "Epoch 642...\n",
      "Train loss: 0.08085363417863846\n",
      "Test loss: 0.12479079663753509\n",
      "Epoch 643...\n",
      "Train loss: 0.0809177479147911\n",
      "Test loss: 0.1189519464969635\n",
      "Epoch 644...\n",
      "Train loss: 0.08241994202136993\n",
      "Test loss: 0.1286076694726944\n",
      "Epoch 645...\n",
      "Train loss: 0.07988893896341324\n",
      "Test loss: 0.1258779540657997\n",
      "Epoch 646...\n",
      "Train loss: 0.08287621259689332\n",
      "Test loss: 0.1290615126490593\n",
      "Epoch 647...\n",
      "Train loss: 0.08285628408193588\n",
      "Test loss: 0.1274600937962532\n",
      "Epoch 648...\n",
      "Train loss: 0.08296348690986634\n",
      "Test loss: 0.11921823620796204\n",
      "Epoch 649...\n",
      "Train loss: 0.08123961210250855\n",
      "Test loss: 0.1352158486843109\n",
      "Epoch 650...\n",
      "Train loss: 0.08499313324689865\n",
      "Test loss: 0.12755277305841445\n",
      "Epoch 651...\n",
      "Train loss: 0.08246970534324646\n",
      "Test loss: 0.12467511892318725\n",
      "Epoch 652...\n",
      "Train loss: 0.08171810448169708\n",
      "Test loss: 0.1262853264808655\n",
      "Epoch 653...\n",
      "Train loss: 0.08084232687950134\n",
      "Test loss: 0.1291268989443779\n",
      "Epoch 654...\n",
      "Train loss: 0.08381501019001007\n",
      "Test loss: 0.1355793982744217\n",
      "Epoch 655...\n",
      "Train loss: 0.08067365646362305\n",
      "Test loss: 0.12615689933300017\n",
      "Epoch 656...\n",
      "Train loss: 0.07925709873437882\n",
      "Test loss: 0.13382371962070466\n",
      "Epoch 657...\n",
      "Train loss: 0.07925754576921464\n",
      "Test loss: 0.12734367400407792\n",
      "Epoch 658...\n",
      "Train loss: 0.08275747507810592\n",
      "Test loss: 0.1327325075864792\n",
      "Epoch 659...\n",
      "Train loss: 0.07893648236989975\n",
      "Test loss: 0.1311940595507622\n",
      "Epoch 660...\n",
      "Train loss: 0.08124009013175965\n",
      "Test loss: 0.12330241799354554\n",
      "Epoch 661...\n",
      "Train loss: 0.08108846336603165\n",
      "Test loss: 0.13018375933170317\n",
      "Epoch 662...\n",
      "Train loss: 0.08167628169059754\n",
      "Test loss: 0.12783878445625305\n",
      "Epoch 663...\n",
      "Train loss: 0.080557541847229\n",
      "Test loss: 0.12964263409376145\n",
      "Epoch 664...\n",
      "Train loss: 0.0825301904976368\n",
      "Test loss: 0.12871193438768386\n",
      "Epoch 665...\n",
      "Train loss: 0.08055470794439316\n",
      "Test loss: 0.13316991031169892\n",
      "Epoch 666...\n",
      "Train loss: 0.0786537304520607\n",
      "Test loss: 0.13445240408182144\n",
      "Epoch 667...\n",
      "Train loss: 0.08012216091156006\n",
      "Test loss: 0.12036172151565552\n",
      "Epoch 668...\n",
      "Train loss: 0.08340699642896653\n",
      "Test loss: 0.1335255414247513\n",
      "Epoch 669...\n",
      "Train loss: 0.0796359857916832\n",
      "Test loss: 0.12226628214120865\n",
      "Epoch 670...\n",
      "Train loss: 0.07918751746416092\n",
      "Test loss: 0.13079589009284973\n",
      "Epoch 671...\n",
      "Train loss: 0.08011520773172379\n",
      "Test loss: 0.1329854965209961\n",
      "Epoch 672...\n",
      "Train loss: 0.07976027369499207\n",
      "Test loss: 0.12889377474784852\n",
      "Epoch 673...\n",
      "Train loss: 0.07922046929597855\n",
      "Test loss: 0.1256275877356529\n",
      "Epoch 674...\n",
      "Train loss: 0.07961710005998611\n",
      "Test loss: 0.13627442717552185\n",
      "Epoch 675...\n",
      "Train loss: 0.07793091654777527\n",
      "Test loss: 0.13448581993579864\n",
      "Epoch 676...\n",
      "Train loss: 0.08051713645458221\n",
      "Test loss: 0.13182134926319122\n",
      "Epoch 677...\n",
      "Train loss: 0.08138064920902252\n",
      "Test loss: 0.12379086911678314\n",
      "Epoch 678...\n",
      "Train loss: 0.07992010235786438\n",
      "Test loss: 0.12791076749563218\n",
      "Epoch 679...\n",
      "Train loss: 0.08064333140850068\n",
      "Test loss: 0.13464727699756623\n",
      "Epoch 680...\n",
      "Train loss: 0.08064218252897262\n",
      "Test loss: 0.12972269356250762\n",
      "Epoch 681...\n",
      "Train loss: 0.07950695991516113\n",
      "Test loss: 0.13034398406744002\n",
      "Epoch 682...\n",
      "Train loss: 0.07984023034572602\n",
      "Test loss: 0.12224621772766113\n",
      "Epoch 683...\n",
      "Train loss: 0.08103903234004975\n",
      "Test loss: 0.1264502465724945\n",
      "Epoch 684...\n",
      "Train loss: 0.0797068578004837\n",
      "Test loss: 0.13898641467094422\n",
      "Epoch 685...\n",
      "Train loss: 0.08105861335992813\n",
      "Test loss: 0.1275232121348381\n",
      "Epoch 686...\n",
      "Train loss: 0.08132034838199616\n",
      "Test loss: 0.13080841451883315\n",
      "Epoch 687...\n",
      "Train loss: 0.0798084345459938\n",
      "Test loss: 0.13439735770225525\n",
      "Epoch 688...\n",
      "Train loss: 0.07966498792171478\n",
      "Test loss: 0.13597504496574403\n",
      "Epoch 689...\n",
      "Train loss: 0.07861460834741592\n",
      "Test loss: 0.130750834941864\n",
      "Epoch 690...\n",
      "Train loss: 0.07828969210386276\n",
      "Test loss: 0.12910841703414916\n",
      "Epoch 691...\n",
      "Train loss: 0.08106518030166626\n",
      "Test loss: 0.12178939282894134\n",
      "Epoch 692...\n",
      "Train loss: 0.0790375229716301\n",
      "Test loss: 0.1331232562661171\n",
      "Epoch 693...\n",
      "Train loss: 0.07986095041036606\n",
      "Test loss: 0.13435473740100862\n",
      "Epoch 694...\n",
      "Train loss: 0.07588260978460312\n",
      "Test loss: 0.12726277559995652\n",
      "Epoch 695...\n",
      "Train loss: 0.077877317070961\n",
      "Test loss: 0.1315161854028702\n",
      "Epoch 696...\n",
      "Train loss: 0.07999119341373444\n",
      "Test loss: 0.13737439811229707\n",
      "Epoch 697...\n",
      "Train loss: 0.07686723232269287\n",
      "Test loss: 0.1254014104604721\n",
      "Epoch 698...\n",
      "Train loss: 0.08039351314306259\n",
      "Test loss: 0.1273086339235306\n",
      "Epoch 699...\n",
      "Train loss: 0.07965430974960327\n",
      "Test loss: 0.12983961701393126\n",
      "Epoch 700...\n",
      "Train loss: 0.07822782158851624\n",
      "Test loss: 0.12795986235141754\n",
      "Epoch 701...\n",
      "Train loss: 0.07763411462306977\n",
      "Test loss: 0.12674034535884857\n",
      "Epoch 702...\n",
      "Train loss: 0.07944432348012924\n",
      "Test loss: 0.12703189849853516\n",
      "Epoch 703...\n",
      "Train loss: 0.0806929811835289\n",
      "Test loss: 0.1256329372525215\n",
      "Epoch 704...\n",
      "Train loss: 0.07893799811601639\n",
      "Test loss: 0.12995263636112214\n",
      "Epoch 705...\n",
      "Train loss: 0.07822512716054916\n",
      "Test loss: 0.12188373506069183\n",
      "Epoch 706...\n",
      "Train loss: 0.07780901923775672\n",
      "Test loss: 0.12593550533056258\n",
      "Epoch 707...\n",
      "Train loss: 0.07696233987808228\n",
      "Test loss: 0.1346796065568924\n",
      "Epoch 708...\n",
      "Train loss: 0.07817055284976959\n",
      "Test loss: 0.12744075208902358\n",
      "Epoch 709...\n",
      "Train loss: 0.07800487041473389\n",
      "Test loss: 0.12265084832906722\n",
      "Epoch 710...\n",
      "Train loss: 0.07661279737949371\n",
      "Test loss: 0.12464232444763183\n",
      "Epoch 711...\n",
      "Train loss: 0.07875411033630371\n",
      "Test loss: 0.13372595757246017\n",
      "Epoch 712...\n",
      "Train loss: 0.07747610956430435\n",
      "Test loss: 0.1291410282254219\n",
      "Epoch 713...\n",
      "Train loss: 0.0792725557088852\n",
      "Test loss: 0.11724175214767456\n",
      "Epoch 714...\n",
      "Train loss: 0.07711809575557708\n",
      "Test loss: 0.12756284177303315\n",
      "Epoch 715...\n",
      "Train loss: 0.07862927168607711\n",
      "Test loss: 0.13039787411689757\n",
      "Epoch 716...\n",
      "Train loss: 0.07827040076255798\n",
      "Test loss: 0.12462005913257598\n",
      "Epoch 717...\n",
      "Train loss: 0.07732033103704453\n",
      "Test loss: 0.12561367303133011\n",
      "Epoch 718...\n",
      "Train loss: 0.0783747997879982\n",
      "Test loss: 0.12756658345460892\n",
      "Epoch 719...\n",
      "Train loss: 0.07840982347726821\n",
      "Test loss: 0.13104449808597565\n",
      "Epoch 720...\n",
      "Train loss: 0.07552941530942917\n",
      "Test loss: 0.13232906460762023\n",
      "Epoch 721...\n",
      "Train loss: 0.07932764947414399\n",
      "Test loss: 0.12642409950494765\n",
      "Epoch 722...\n",
      "Train loss: 0.08060005754232406\n",
      "Test loss: 0.1259854257106781\n",
      "Epoch 723...\n",
      "Train loss: 0.0759040230512619\n",
      "Test loss: 0.13955722004175186\n",
      "Epoch 724...\n",
      "Train loss: 0.07856856092810631\n",
      "Test loss: 0.13567306399345397\n",
      "Epoch 725...\n",
      "Train loss: 0.07780434608459473\n",
      "Test loss: 0.13205620348453523\n",
      "Epoch 726...\n",
      "Train loss: 0.07598019570112229\n",
      "Test loss: 0.129412642121315\n",
      "Epoch 727...\n",
      "Train loss: 0.07911806106567383\n",
      "Test loss: 0.1201574295759201\n",
      "Epoch 728...\n",
      "Train loss: 0.07646226823329925\n",
      "Test loss: 0.12872592508792877\n",
      "Epoch 729...\n",
      "Train loss: 0.07719611793756485\n",
      "Test loss: 0.1338103234767914\n",
      "Epoch 730...\n",
      "Train loss: 0.07680913597345353\n",
      "Test loss: 0.1329028680920601\n",
      "Epoch 731...\n",
      "Train loss: 0.07686502605676651\n",
      "Test loss: 0.13568563461303712\n",
      "Epoch 732...\n",
      "Train loss: 0.07891448140144348\n",
      "Test loss: 0.12647305727005004\n",
      "Epoch 733...\n",
      "Train loss: 0.07725646287202835\n",
      "Test loss: 0.13713543117046356\n",
      "Epoch 734...\n",
      "Train loss: 0.07888108611106873\n",
      "Test loss: 0.12993135303258896\n",
      "Epoch 735...\n",
      "Train loss: 0.07852581679821015\n",
      "Test loss: 0.12774809300899506\n",
      "Epoch 736...\n",
      "Train loss: 0.07795537441968918\n",
      "Test loss: 0.13095878064632416\n",
      "Epoch 737...\n",
      "Train loss: 0.07950532376766205\n",
      "Test loss: 0.13217040300369262\n",
      "Epoch 738...\n",
      "Train loss: 0.07783832013607025\n",
      "Test loss: 0.1326956868171692\n",
      "Epoch 739...\n",
      "Train loss: 0.07708146035671234\n",
      "Test loss: 0.12905481308698655\n",
      "Epoch 740...\n",
      "Train loss: 0.07645374298095703\n",
      "Test loss: 0.12553142756223679\n",
      "Epoch 741...\n",
      "Train loss: 0.07812897264957427\n",
      "Test loss: 0.13064438998699188\n",
      "Epoch 742...\n",
      "Train loss: 0.07572835206985473\n",
      "Test loss: 0.13359155058860778\n",
      "Epoch 743...\n",
      "Train loss: 0.07722568973898887\n",
      "Test loss: 0.12422344982624053\n",
      "Epoch 744...\n",
      "Train loss: 0.07552703633904458\n",
      "Test loss: 0.1349438488483429\n",
      "Epoch 745...\n",
      "Train loss: 0.07727893888950348\n",
      "Test loss: 0.1308334305882454\n",
      "Epoch 746...\n",
      "Train loss: 0.07617855995893479\n",
      "Test loss: 0.13818127661943436\n",
      "Epoch 747...\n",
      "Train loss: 0.0741576075553894\n",
      "Test loss: 0.133558189868927\n",
      "Epoch 748...\n",
      "Train loss: 0.07408570379018783\n",
      "Test loss: 0.12528028935194016\n",
      "Epoch 749...\n",
      "Train loss: 0.07657246977090836\n",
      "Test loss: 0.1298847645521164\n",
      "Epoch 750...\n",
      "Train loss: 0.0777916419506073\n",
      "Test loss: 0.13693350851535796\n",
      "Epoch 751...\n",
      "Train loss: 0.07573564380407333\n",
      "Test loss: 0.12688325792551042\n",
      "Epoch 752...\n",
      "Train loss: 0.0773921811580658\n",
      "Test loss: 0.12480045557022094\n",
      "Epoch 753...\n",
      "Train loss: 0.07720697075128555\n",
      "Test loss: 0.11830534636974335\n",
      "Epoch 754...\n",
      "Train loss: 0.07547184795141221\n",
      "Test loss: 0.12496563792228699\n",
      "Epoch 755...\n",
      "Train loss: 0.07608556151390075\n",
      "Test loss: 0.13490024209022522\n",
      "Epoch 756...\n",
      "Train loss: 0.07649383991956711\n",
      "Test loss: 0.12799035757780075\n",
      "Epoch 757...\n",
      "Train loss: 0.07784368574619294\n",
      "Test loss: 0.12987442314624786\n",
      "Epoch 758...\n",
      "Train loss: 0.07507928684353829\n",
      "Test loss: 0.128156116604805\n",
      "Epoch 759...\n",
      "Train loss: 0.07375570178031922\n",
      "Test loss: 0.12606805711984634\n",
      "Epoch 760...\n",
      "Train loss: 0.0749786365032196\n",
      "Test loss: 0.13883072137832642\n",
      "Epoch 761...\n",
      "Train loss: 0.07689928591251373\n",
      "Test loss: 0.13230381309986114\n",
      "Epoch 762...\n",
      "Train loss: 0.07748307049274444\n",
      "Test loss: 0.13631503134965897\n",
      "Epoch 763...\n",
      "Train loss: 0.0771318556368351\n",
      "Test loss: 0.13221345990896224\n",
      "Epoch 764...\n",
      "Train loss: 0.07556975960731506\n",
      "Test loss: 0.13224126547574996\n",
      "Epoch 765...\n",
      "Train loss: 0.07722538441419602\n",
      "Test loss: 0.13197961449623108\n",
      "Epoch 766...\n",
      "Train loss: 0.07636683136224746\n",
      "Test loss: 0.13539354205131532\n",
      "Epoch 767...\n",
      "Train loss: 0.07571537986397743\n",
      "Test loss: 0.13658492863178254\n",
      "Epoch 768...\n",
      "Train loss: 0.0774064788222313\n",
      "Test loss: 0.12471915632486344\n",
      "Epoch 769...\n",
      "Train loss: 0.07319318920373917\n",
      "Test loss: 0.12734992057085037\n",
      "Epoch 770...\n",
      "Train loss: 0.07374324649572372\n",
      "Test loss: 0.12677043229341506\n",
      "Epoch 771...\n",
      "Train loss: 0.07559608638286591\n",
      "Test loss: 0.1315752610564232\n",
      "Epoch 772...\n",
      "Train loss: 0.07437535792589188\n",
      "Test loss: 0.1303581938147545\n",
      "Epoch 773...\n",
      "Train loss: 0.07665757179260253\n",
      "Test loss: 0.1336399495601654\n",
      "Epoch 774...\n",
      "Train loss: 0.07406653121113776\n",
      "Test loss: 0.13167213052511215\n",
      "Epoch 775...\n",
      "Train loss: 0.07339602217078209\n",
      "Test loss: 0.1376705914735794\n",
      "Epoch 776...\n",
      "Train loss: 0.07511512368917465\n",
      "Test loss: 0.13855300843715668\n",
      "Epoch 777...\n",
      "Train loss: 0.07625614315271377\n",
      "Test loss: 0.1303589954972267\n",
      "Epoch 778...\n",
      "Train loss: 0.07700260877609252\n",
      "Test loss: 0.12957178354263305\n",
      "Epoch 779...\n",
      "Train loss: 0.07651254564523696\n",
      "Test loss: 0.1349218785762787\n",
      "Epoch 780...\n",
      "Train loss: 0.0760465231537819\n",
      "Test loss: 0.1321496307849884\n",
      "Epoch 781...\n",
      "Train loss: 0.0743043702840805\n",
      "Test loss: 0.13083418607711791\n",
      "Epoch 782...\n",
      "Train loss: 0.0744597777724266\n",
      "Test loss: 0.138904669880867\n",
      "Epoch 783...\n",
      "Train loss: 0.07483075380325317\n",
      "Test loss: 0.12829872518777846\n",
      "Last save on 483 epoch\n"
     ]
    }
   ],
   "source": [
    "last_save = 0\n",
    "epoch = 0\n",
    "while epoch - last_save < 300:\n",
    "    epoch += 1\n",
    "    print(f'Epoch {epoch}...')\n",
    "\n",
    "    loss = train(model, loss_fn, optim)\n",
    "    print(f'Train loss: {loss}')\n",
    "\n",
    "    loss = test(model, loss_fn)\n",
    "    print(f'Test loss: {loss}')\n",
    "\n",
    "    if loss < best:\n",
    "        torch.save(model,'../model.pt')\n",
    "        best = loss\n",
    "        last_save = epoch\n",
    "        print('Model saved')\n",
    "\n",
    "print(f'Last save on {last_save} epoch')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-06T17:06:41.020684600Z",
     "start_time": "2024-10-06T17:06:06.573280400Z"
    }
   },
   "id": "a864d29d0ab2088"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12071859836578369\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('../model.pt')\n",
    "print(test(model, loss_fn))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-06T17:06:48.480457500Z",
     "start_time": "2024-10-06T17:06:48.418281800Z"
    }
   },
   "id": "790b9e6f33f035f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
